[![English Google Webmaster Central office-hours hangout](https://i.ytimg.com/vi/u7JY_Tz2c4U/hqdefault.jpg)](https://www.youtube.com/watch?v=u7JY_Tz2c4U)

## English Google Webmaster Central office-hours hangout

Join us for a Google Webmaster Central office hours hangout on June 25, 4pm CET 

https://www.timeanddate.com/worldclock/fixedtime.html?iso=20190625T16&p1=268



This session is open to anything webmaster related like crawling, indexing, mobile sites, internationalization, duplicate content, Sitemaps, Search Console, pagination, duplicate content, multi-lingual/multi-regional sites, etc. Add your questions at https://www.youtube.com/user/GoogleWebmasterHelp/community



To join live, watch out for the link here once the event starts, and use a webcam + headset. Feel free to drop by - we welcome webmasters of all levels!



#### [0:00:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=0) |  JOHN MUELLER: All right. Welcome, everyone, to

today's Webmaster Central Office Hours Hangout. My name is John Mueller. I'm a webmaster trends analyst here at Google in Switzerland. And part of what we do are this Office Hour Hangouts, where webmasters and publishers can jump in and ask questions around their website and web search. As always, it looks a bunch of questions were submitted. If any of you want to get started, though, with the first question, feel free to jump on in now.  

#### [0:00:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=30) |  THOMAS: Well, John, I'll take a stab

at it. My name's Tom Goring. I have been doing this for about 15 years, but just a hobby. And my niche is the Navy, the United States Navy. And as an example, one search term, navy PRT standards. I just put it-- I'm trying to put it in the chat-- it delivers the first, or one page for the first seven results.  

#### [0:01:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=60) |  And that's not common that I see

that, but it does happen in a few areas of my niche that has tabular data or charts that are being delivered. And what I'm seeing more and more of now is a number of sites creating a page for each small little bit of information. So in this particular case he's got-- well, it's actually more than seven, but you return seven to describe one thing. I'm seeing this more and more as--  

#### [0:01:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=90) |  is it advantageous? Is it something Google

wants us to do, is to break one page up into multiple pages to make it look like we have more content that what, probably, needs to be delivered on one page? Or is one page a better-- I think you probably know what I'm asking. JOHN MUELLER: Yeah, I think there's no absolute answer there. So on the one hand, we recently rolled out an update to make-- oops, a little bit o background noise there--  

#### [0:02:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=120) |  to make sure that there is a

bit more diversity in the search results. So for the most part, we'll limited to two search results entries per site. There's some cases where we do show more. So some really, I guess, obvious cases are if we can tell that someone is really looking for one specific website, we'll, obviously, try to show more content from that website, because it matches what we think the user is looking for.  

#### [0:02:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=150) |  With regards to whether or not you

should split things up or combine things into fewer pages, ultimately, that's always a balancing act. On the one hand, by splitting things up, you have more types, more pages on your website, which might be better focused. But on the other hand, those pages individually have less value. So if you have, I don't know, 10 of value for your website overall, and you split that across 100 pages,  

#### [0:03:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=180) |  then each of those pages is going

to get a lot less than if you just have 10 pages. So that's kind of what you end up balancing there. And some sites say, well, we have a lot of detailed information. We'll split things up, and we'll try to rank for those kind of niche terms. And other sites focus more on the head terms and say, well, we'll combine more of our information on fewer pages and make those really strong pages within our website so that they rank really well for these more generic terms.  

#### [0:03:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=210) |  So that's something that is almost like

a strategic decision on your side. My general recommendation to the average website is always to reduce the number of pages rather than to increase them. So if you have the choice between making more pages or making fewer pages that are better, and you're torn, like, which way should I go, then I'd recommend focusing more on fewer pages, making those fewer pages a lot stronger.  

#### [0:04:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=240) |  And then, over time, as you see

that you really have the ability to make individual more niche pages-- and maybe that's something to expand on over time. But definitely, in the beginning, instead of creating all of these millions of pages that-- I don't know, "millions," or lots of pages focusing on individual pieces of a problem, then you're usually better off by just having fewer pages that are a lot stronger in search. THOMAS: Thank you.  

#### [0:04:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=270) |  KORAY: Hello, John. I have another question,

if you let me, I want to ask. JOHN MUELLER: Sure. KORAY: OK, it is about multi-language websites. I have websites with seven other languages, and I want to join in Arabic market. And I wonder, if I choose Arabic language as default language of my website, this will change my rankings on Arabic countries or not? JOHN MUELLER: I assume you have the different languages on different URLs?  

#### [0:05:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=300) |  KORAY: Yes. And I want to focus

on Arabic language. And if I choose Arabic language as default of my website, this will boost me or not in Arabic queries? JOHN MUELLER: So by "default," do you mean the [INAUDIBLE] default in the hreflang setting, or-- KORAY: Actually, original language, home page language. JOHN MUELLER: Home page language? Maybe. I don't know. It's hard to say. KORAY: OK.  

#### [0:05:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=330) |  JOHN MUELLER: It's not totally obvious that

that would always work. Usually, if you're not translating the home page, and the home page is one of the pages, usually, which are linked the most from the external web, then the home page has a little bit more weight. KORAY: OK. JOHN MUELLER: So If you have the home page in Arabic, then probably, we'll be able to pick that up. KORAY: It may work. OK. JOHN MUELLER: Yeah. But especially if you have the home page also translated, and with the hflang links between the translated versions, then, for us, they're kind  

#### [0:06:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=360) |  of a cluster of pages that we

would treat similar to the home page. So it might help, but I don't think it would be-- like it's a good recipe for [INAUDIBLE].. KORAY: OK, it's not a major factor, as I can understand. JOHN MUELLER: Yeah. KORAY: And may I ask another question? JOHN MUELLER: Sure. KORAY: OK, it is about internal links. And let's say I have a page with several internal links. And I tag them with nofollow tag. I wonder if this will affect my link juice or not--  

#### [0:06:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=390) |  I mean the link power of the

page. JOHN MUELLER: Yeah, so with the rel=nofollow on the links, you're basically telling us not to pass any signals to that page that you're linking to. KORAY: So it doesn't affect? So JOHN MUELLER: It wouldn't help those pages. KORAY: And also, it doesn't decrease the power of the page, the link power of the page? JOHN MUELLER: I don't think it would work that way, yeah. KORAY: OK, OK, thank you very much.  

#### [0:07:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=420) |  And also, I have other questions, but

I need to stop in here, and I will ask them later with writing. Thank you. JOHN MUELLER: OK, cool. OK, let me jump through some of the questions that were submitted. KORAY: OK. JOHN MUELLER: And if any of you have questions later on, or more comments along the way, feel free to jump on it. And let me just mute you, because there's a little bit of background noise. We have a bunch of URLs with non-Latin characters. Would you recommend that we change the characters,  

#### [0:07:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=450) |  or can Google crawl these URLs with

no problem? We should be able to crawl those URLs with no problem. But something-- I think we've been doing it for a really long time-- that should just work. What are the webmaster guidelines on meta description tag? What's the ideal length? Does Google penalize pages with long meta descriptions? You can make meat descriptions as long as you want. There is no ideal length that we recommend. There is no maximum length. Make them long, make them short.  

#### [0:08:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=480) |  If they're very long, then we will

tend to cut them when it comes to showing them in the search results. If they're very short, we might take pieces of text from the page itself and show those in the snippet as well. But there is no penalty for any of that. There is no-- as far as I know, there is no ranking effect at all from the meta description within the page. Question about the rich results in a search performance or search appearance report.  

#### [0:08:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=510) |  We're seeing that since May 21, the

rich results count goes to 0. This is for a job site, so I'm just paraphrasing the question a little bit. I looked into this a bit with the team to figure out what was happening here, and I'm not sure which site this involves, but I did see it on one of the other job sites as well. And essentially, what happens here is we see different kinds of job listings as, potentially, also  

#### [0:09:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=540) |  being seen as a rich result [INAUDIBLE]..

And depending on how we're showing those job listings, they might be counted for one or the other or for both. And I think looking into another site that had a similar problem like this, or "problem"-- a similar change at least-- we essentially just didn't count that as a rich result anymore because we already counted it as a job listing or job detail entry. So it doesn't make sense to count that twice  

#### [0:09:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=570) |  for different types. So I suspect that

that's what happened there. FAYSAL: John, can I ask you a follow-up on that? JOHN MUELLER: Sure. FAYSAL: So are you saying that rich results is a catch-all at this point? And then, perhaps, later on, you'll parse out different kinds of rich results, and they'll be counted uniquely? And which is also he may be double counting some of them, [INAUDIBLE]? JOHN MUELLER: Yeah, I think at the moment, that might be happening, yeah. That's something that I'm always in discussion  

#### [0:10:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=600) |  with the team with regards to how

we should count different results types. And I have definitely seen that happening, where, if you look at the rich results count, and you look at a specific type as well, then the counts are very similar. And that, to me, points to us double-counting those as different types. So if you look at it overall, that's not visible. But if you look at it by search appearance type, as it's called, then you would see that.  

#### [0:10:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=630) |  And I think that's pretty confusing. I

wish we could make that a little bit clearer. And I'm not quite sure which direction we'll go there. Because it's similar to something that we've touched upon before in the past, in that on the one hand, we want to make it possible for you to have a quick overview of the bigger picture, where something like a rich result type would be useful to see. On the other hand, we also want to make it possible for you to dig in and see, for this specific type  

#### [0:11:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=660) |  of rich result, what were my results

there? And finding the balance and which result type goes where is sometimes a bit tricky. "We have a DIY e-commerce site, and many of our blog how-to articles are now indexed from Google. From looking, I think this has something to do with [? EAT ?] rating guidelines. So these articles are written by builders and people with DIY experience. How can we show this to Google?  

#### [0:11:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=690) |  Do we set up an author or

biography page for each article?" And all of these things. So I think, first of all, it might just be a matter of phrasing it. But one of the things you mentioned there on the one hand is you said that these pages are de-indexed by Google. That's the first thing I would double-check. If you're not seeing traffic to those pages, I would double-check to see if these pages are actually still  

#### [0:12:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=720) |  indexed by Google or if they're not

indexed by Google, mainly because if a page is not indexed by Google, then most likely, it's more of a technical issue, that maybe there a no-index there, or there's a redirect, or rel=canonical, or something that's blocking indexing of that page. And those issues are usually pretty easy to narrow down and to fix. So if you're really seeing that this is an indexing issue, then I would look for technical issues. You can tell if it's an indexing issue by taking the URL  

#### [0:12:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=750) |  and just searching for that URL directly.

Then you'll see right away, is this page found in search or not? If it's found in search, then it's not an indexing issue. If it's not found in search, then it might be an indexing issue, and that's something, like I said, with the index coverage report, you can probably drill down and figure out where that's coming from. If it's not an indexing issue. So if these pages are actually indexed, and they're just not ranking as well, then all of the things  

#### [0:13:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=780) |  you've talked about, I think, are good

steps to take. So anything that you can really do to increase the quality of your website overall, that's something that you could work on, that you could work to improve. So that could be things like explaining where these authors are from, where this content is from, how your content could be seen as trustworthy or not. In general, with something like a DIY blog or a DIY e-commerce site, I suspect that the EAT guidelines are not  

![](https://i.ytimg.com/vi/u7JY_Tz2c4U/hq1.jpg)



#### [0:13:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=810) |  that critical for your site. Because it's

not that you have to have a PhD in DIY to be a trustworthy source on how to paint something. So from that point of view, I'd caution against going too deep too quickly and trying to find ways that you can just generally improve the quality of your website overall.  

#### [0:14:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=840) |  And this is sometimes easier said than

done. It's really something where, if you've been working on your website for a long time, then you'll feel like, my website is fantastic. How dare anyone come and say my website needs to be improved in quality? But sometimes, getting input from other people who also run web sites, or getting input from people who are interested in the topics of your website but not associated with your particular website, that can be really useful, to kind of get that feedback and think about ways that you can really  

#### [0:14:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=870) |  take things to the next level. A

question about event markup. We have one customer who marks up events with structured data markup, but their event-rich snippets don't show up in search. Their competitors have exactly the same markup, but their rich results show up. What could be happening here? So when it comes to rich results, we have multiple things that need to align in order for us to show them. On the one hand, it has to be technically implemented  

#### [0:15:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=900) |  properly. You can see that by using

a testing tool-- the Rich Results Testing Tool, or the Structured Data Testing Tool-- to see if it's properly implemented. If it's set up in exactly the same way as other people have it set up, then probably, the technical thing is a quick check that you can see and see that it works. The second thing is it needs to comply with our policies, and the policies are in the developer documentation. I double-check those. And the third thing, which is kind of the tricky one,  

#### [0:15:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=930) |  is we need to be sure that

the page or the website is of high quality overall so that we can really trust this particular markup and show that, highlight that in the search results. So if technically everything is OK, if you're OK with the policies, then it's probably worth taking a step back and thinking about quality overall, which is similar to the previous question as well. So sometimes, that's kind of tricky.  

#### [0:16:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=960) |  And another question about EAT, a question

possibly relating to EAT. We have a customer whose product pages are well-optimized and contained both brief information about products and the ability to purchase. These pages rank number one for informational queries, but frequently don't rank at all for purchase intent queries, while their competitors with similar but less-optimized pages do. Is it possible that the portion of the site  

#### [0:16:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=990) |  after the Add to Cart link has

an effect on ranking for purchase intent queries? Their Add to Cart leads to a different subdomain, and the cart page shows a 403 error to bots and can't be access directly, to prevent abuse. Could this be a factor? So we don't need to be able to crawl the Add to Cart pages. So from that point of view, if you're blocking those  

#### [0:17:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1020) |  with the robots.txt, if you're blocking those

in other ways, that's perfectly fine. That's up to you. When it comes to evaluating the quality of a website overall, we evaluate the pages that we can index. So anything that is crawlable, that returns content, that is not blocked by no-index, that's something that we can take into account to evaluate the quality of the website, and that's something that primarily plays a role in how we rank those pages for individual queries.  

#### [0:17:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1050) |  So with that said, I think you

don't need to worry about the Add to Cart section being blocked and on a different subdomain. That's perfectly fine. That's totally up to you. It might make it harder for things like auto-complete, but ultimately, that's something you have to work with regards to usability, and it's not something that would play a role with regards to SEO. With regards to ranking for purchase intent queries,  

#### [0:18:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1080) |  that's really hard to say without knowing

some examples of what you're seeing. So in general, we would kind of see these as being similar queries, and we try to match the keywords and the synonyms to see which of these pages belong together. And usually, e-commerce sites are pretty obvious in that these are things that you can buy, and there is information about how to buy it, how to get it delivered, all of that,  

#### [0:18:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1110) |  to tell us that this is an

e-commerce site. So it feels a bit weird that we wouldn't be able to pick that up properly. So what my recommendation here would be is, maybe, to go to one of the webmaster forums and chat with some peers to see how they would see it. Is there anything specific for individual queries that is not working well, that might be escalated, maybe, to us directly, so that we could take a look at specific queries and specific results on your site?  

#### [0:19:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1140) |  Question about XML Site Maps. "If you

generate a dynamic XML site map for your website that's always up to date with new, modified, and deleted URLs, let's see. If you, in addition, keep the correct date and timestamp for each single URL when added or modified, will that help Google to faster read and index new and modified content on the website compared to not using XML Site Maps?"  

#### [0:19:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1170) |  Yes, definitely. So within an XML Site

Map, you can list all of the URLs that are on your website that you want to have indexed. There are different attributes that you can specify for each individual URL in the sitemap file. The one that we care about the most is the last modification date. So with that last modification date, if you tell us this page has significantly changed on this particular date, we can double-check our records and say, oh, we haven't crawled that page since that date,  

#### [0:20:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1200) |  and you're telling us it has just

recently changed, so we should go and crawl that as quickly as possible. So an XML site map file definitely helps us there. When it comes to really small web sites, we can generally re-crawl the whole website fairly quickly, so that's less of an issue. But especially for larger sites, where maybe some product on your site has changed, and that's linked through different levels of categories, than us finding that product will take a bit of time.  

#### [0:20:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1230) |  So we wouldn't be able to find

that change as quickly as we might otherwise. And with the site map file, you can tell us specifically, that product back there has changed. Go check it out. So definitely, the last modification date is the one that we watch out for. The change frequency and the priority in site map files we currently don't use at all for web search, just because we found that it's not really as useful as we initially thought when we worked on the site maps standard.  

#### [0:21:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1260) |  "At the moment, we're using five IPs

for five SSL Domains, we're switching to a new system where we can serve all five domains on one IP. Do you see any problem?" That's perfectly fine. You can serve SSL certificates or TLS certificates for different websites on the same IP address. That's certainly possible. I don't see any problem at all with regards  

#### [0:21:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1290) |  to Google for that setup. Lots of

sites do that. Then, a question. "So I look into-- I think this is a site that have pinged me on Twitter as well and posted on Reddit a bunch of details. So there is a site that dropped heavily in rankings after a subdomain had been hacked. And they fixed it, and they're still dropped in rankings.  

#### [0:22:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1320) |  If that could be something do to

Google's pirate algorithm or one of the other algorithms?" I took a look into that a little bit to see what was specifically happening there. And it's really rare, but sometimes it happens that a website is hacked in a way that we think this website has significantly changed. And when that hack is resolved, it still takes quite a bit of time for everything  

#### [0:22:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1350) |  to settle down again. And I think

that's what's happening here. It's not that there's anything exotic happening with this website, anything manual that you need to unhook, but more that our algorithms are just in this cautious role of, oh, it changed in a really bad way. And we need to be sure that the current change back to a good website is actually a stable change. And sometimes that takes a bit of time. It's pretty rare nowadays.  

#### [0:23:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1380) |  It used to be more of an

issue in the past, and nowadays, we're able to recrawl and reprocess the site after it's been hacked fairly quickly. But it looks like it's still a bit slow here in this particular case. I also passed this on to the team to take a look to see if there is something that we could do to improve the speed of our algorithms in cases like this. Then a question about manual actions, reconsiderations.  

#### [0:23:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1410) |  Does the domain history affect the priority

of reviewing a request for removing manual action for unnatural incoming links? So there's a little bit more details there. The general answer is, no. It's not the case that we look at the domain history and say, oh, they were doing sneaky things five years ago. Therefore, we will be slower, or we will be more critical when they do a reconsideration  

#### [0:24:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1440) |  request now. However, if we see that

a site goes back and forth regularly-- like, for example, with unnatural links, they build a whole bunch of unnatural links. They get a manual action. They disavow all these links. We resolve the manual action. Then they remove the disavow file and keep building unnatural links again, and repeating the cycle multiple times, then that's something that the web spam team will look at and say, well, come on, you're playing games with us.  

#### [0:24:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1470) |  So that's not something where it's worth

our time to prioritize focusing on your website over all of the other legitimate sites that are accidentally running into spamming issues. So that's the situation where I would imagine the web spam team would be a little bit more critical and perhaps take a bit more time to really make sure that, actually, this issue is resolved completely and it won't just be another back and forth again. But overall, if there was a manual action a while back,  

#### [0:25:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1500) |  and you have a new one now

because of something accidental that happened, that's not going to affect it. It's not the case that we will look at the history of the site and say, oh, this site was on our blacklist five or six years ago, and therefore we need to be really critical with it this time. That's not how we do it. A question about a Google search on workday jobs.  

#### [0:25:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1530) |  "So I think this is also kind

of specific. It goes into, well, these jobs are listed on multiple sites. Why do you choose to show someone else's site instead of my site for this particular job listing?" And essentially, what happens here is, like with lots of things in search, we often see the same listing on multiple sites. And we have to pick which ones we show in search.  

#### [0:26:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1560) |  And there are lots of factors that

we use in determining the ranking of individual sites. And similarly, when it comes to the same job listing, we try to pick one of these job pages to show. In particular, if we can tell that the listing is exactly the same, then it doesn't make sense to show that listing multiple times in the search results, because users have already seen one of those. So that's, I'd say, working as intended, in that we recognize  

#### [0:26:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1590) |  these are duplicates, so we'll filter all

of them out except for one of them. Can images loaded via a lazy load script be added to structured data and the site map without causing any issues? Yes, of course. So if you're using lazy loading to load images, so lazy loading is a way of embedding images on a page so that by default, when you open the page, they're not loaded, but when you scroll to that part of the page  

![](https://i.ytimg.com/vi/u7JY_Tz2c4U/hq2.jpg)



#### [0:27:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1620) |  where the image is embedded, it'll be

loaded then, which makes the page load a little bit faster. If the images are embedded in a way that we can recognize that they're there-- so there are different ways to do lazy loading. And if you do that in a way that Google Bot can pick up, then you can use them like any other image on a page. So that can be within structured data. If you have, for example, article markup and you highlight this image as the primary image for the article.  

#### [0:27:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1650) |  You can use image site maps to

tell us this image is located on this landing page. All of these normal uses of images, they're all possible. Regarding our unnatural link penalty from the last video, we haven't received any sample links. So can you tell the team to take a look again? I took a quick look and also sent that onto the team to double-check that. I am not seeing anything specific there.  

#### [0:28:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1680) |  So from what I noticed there is

that the site was building a lot of links using expired domains, where, essentially, the old content of the domain was loaded back, and links were added to those domains. And sometimes we see this happening at a really, really large scale where there will be thousands or tens of thousands of expired domains where these links are embedded and used in various sneaky ways.  

#### [0:28:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1710) |  And what sometimes happens-- I don't know

if this is specific to your site-- but what I've sometimes seen is that the Web Spam Team will say, well, we're seeing that this website has been building links in this particular really sneaky way, and Search Console shows a sample of those links. But we assume that based on this sample, the webmaster can recognize that this is a pattern that applies to their website, and they can go off on their own and find other sites that they built  

#### [0:29:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1740) |  or other links that they've built in

a similar way. So for example, with expired domains, when we see that happening, we don't have to show you all of the expired domains in Search Console. You know where you've built these expired domains, or your SEO has probably been keeping track of all of those expired domains, where they're putting up similar to old content and just adding links to that. So that's something where you can use other tools as well  

#### [0:29:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1770) |  to dig into more details and really

clean that up completely. Again, I don't know if this is specific to your website. It is something that we have seen in very similar cases. So that's one thing where I'd recommend going past, a little bit, just what's available in Search Console and thinking about the patterns that you found there. What kind of links are you seeing there that are problematic that need to be cleaned up, and where might you find other similar links  

#### [0:30:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1800) |  as well that you could clean up

proactively to make sure that it's really cleaned up? I've also pinged the Web Spam team to double-check to make sure that you're really kind of in this state and not that there's something where maybe something from a manual review got missed and we should actually resolve this already. "Do stock photos have an impact on SEO rankings for your blog? Do original photos have an impact?  

#### [0:30:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1830) |  What are the best practices regarding stock

photos?" So stock photos are perfectly fine. With regards to normal web search ranking, we don't take into account the photos that you put on your pages and apply those for the normal web search ranking. Where people are searching for text, and we show the textual search results, you can use stock photos to improve the quality overall of your web pages. That's perfectly fine.  

#### [0:31:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1860) |  When it comes to image search, we

do try to recognize when the same photo is used across multiple sites, and try to just pick one of those, and use that for indexing. So that's something where stock photos probably don't make much of a difference with regards to image search, because we've already seen those stock photos somewhere else on the web, and we're probably showing those other places in the image search results. And for the most part, I think that's kind of to be expected.  

#### [0:31:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1890) |  If you're taking a stock photo and

putting it on your pages, then why would we highlight your pages as the one landing page for that particular stock photo? That doesn't really make that much sense. On the other hand, if you have original photos, then we can definitely show those in the image search. But I think ultimately, it comes down to what it is that you want to achieve. And if image search is not a priority for your website, if you're using these images to make your pages look better,  

#### [0:32:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1920) |  then you're primarily ranking in web search.

And for that, that might be perfectly fine. "I'm running into issues where the search console robots testing tool is showing that I have URL parameters blocked that are getting crawled by Googlebot." I started a thread in the forum. I took a quick look at this briefly with the team.  

#### [0:32:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1950) |  And one of the things I notice

here is you have a really complex URL structure, and you have a really complex robots.txt fie. So the example URL that you specified there is one that we did not crawl. So I suspect, perhaps, your server is logging certain requests in certain ways. For example, if you're using URL rewriting in specific ways, then often, the server will log these under a different URL in the server log. And maybe you're seeing that.  

#### [0:33:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=1980) |  But I think my number one feedback

there would be to make a significantly simpler robots.txt file. So you have multiple parameters. You have multiple wildcards in a lot of these directives. And that makes it really, really hard for you to debug what is actually happening. And I really recommend trying to find a way to significantly simplify the robots.txt files, because they're there misleadingly complex sometimes. Where you think it's just a collection of text lines,  

#### [0:33:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2010) |  how complicated it can be to understand?

But robots.txt, especially with complicated, long URLs, it can get really tricky. So I'd focus more on making things simpler and double check the way that you're logging the crawling to make sure that you're really looking at issues that are real issues that you need to worry about and not something that, essentially, is just confusing because of a complicated URL structure that  

#### [0:34:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2040) |  is logged in a slightly different way

that doesn't map exactly to the robots.txt line that you're expecting. Is the HTML, CSS validity of a document a ranking signal? No. I don't know for sure, but I would imagine that most pages on the web have invalid HTML from a theoretical point of view. And they work well in a browser.  

#### [0:34:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2070) |  They work well for users, for the

most part. And I don't think it would make sense for our algorithms to say, well, you're not following these strict guidelines with regards to HTML validity, therefore, you will be ranked lower, because from a user point of view, they might match their needs perfectly. "The canonical bug is still at large." So I double-checked with the team on this as well.  

#### [0:35:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2100) |  So I just-- I guess on the

background, this is something that is not that canonical bug that we had, what was it, in May at some point, but rather as something unique with regards to these websites. But I've checked with the team as well to see what we can do to make that a little bit clearer, I think what is probably happening here is you have different car dealerships. And they're sharing inventory.  

#### [0:35:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2130) |  And our systems are probably looking at

that and saying, well, this page, or this specific entry that you have here, is available on a bunch of other domains. So maybe these domains are, essentially, the same. Which is not the case, because these are individual dealerships. But our algorithms might be looking at that from a bigger-picture view and saying, well, a lot of these pages are the same as the pages here. Therefore, maybe the whole web sites are the same. But I would definitely see with a team  

#### [0:36:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2160) |  to see what we can do to

make that a little bit clearer, to split these sites up and make them index a little individually again. Let's see. A question about the last update date. Some parts of my content are updated automatically twice a day. Essentially, should I use the last update date for that? You can use that. I don't think you'd see much of a change  

#### [0:36:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2190) |  with regards to search like crawling, indexing

or ranking-wise if you changed the date or the timestamp on a page on a regular basis. For example, weather report page is probably updated every couple of minutes, or every hour at least, but that doesn't mean that these pages suddenly rank a lot better if they have the date stamp that matches the last change that they made on the page. For users, that probably makes a lot of sense,  

#### [0:37:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2220) |  but I wouldn't expect this to play

a role in search. "Our website has subscription-only content which we want to expose in full to Google and shown in limited extract to non-subscribed users. The rich results test and mobile friendly test use the same user agent as Google. The IP address is also resolved through the same Googlebot domain using reverse DNS lookup. Is there any way to allow Googlebot to crawl our site or block these tools?"  

#### [0:37:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2250) |  In short, no. In particular, we explicitly

made these tools to use the Google bot infrastructure so that you can test exactly what Googlebot would see. So that's the goal of these tools, to test exactly how Googlebot would see. So there is no particular way to differentiate between, say, one of these testing tools and a generic Googlebot request. I don't see that happening in the future, though I don't know  

#### [0:38:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2280) |  that might be something where, if you

see a lot of requests for this, then maybe that will change. But I don't see that happening in the future, because we want these testing tools to access a page, how Googlebot would access the page. And if, by design, these requests were different, then you wouldn't be able to debug anymore what Googlebot would be seeing. There seems to be an artificial 50,000-row limit to the Search  

#### [0:38:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2310) |  Console API with regards to, I think,

the Search Analytics API. Is this working as intended, or is there a way to get more data? I think this is working as intended. There is a limit to the amount of data that we have available in Search Console in general, and that's probably visible in the API fairly visibly if you iterate through all of these rows that we have available for individual days.  

#### [0:39:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2340) |  I don't know. Mihai probably has a

bunch of tips on how to get more data out of the API. I don't know-- maybe splitting it up by day, or splitting it up by site pieces. I don't know. What's your experience? MIHAI APERGHIS: So it depends on how you group the data, whether you're getting it by day, by query, by page. So the more granular you want to go, sometimes you'll notice-- it's a small paradox-- the more dimensions you add to your query,  

#### [0:39:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2370) |  sometimes you actually get less data, because

you're basically asking for more private data, so to speak-- more one-user-focused data. So that's sometimes where you see you're getting less rows versus something more broader where we-- but the only bug I know of is that when you're just using for a specific date, you're querying--  

#### [0:40:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2400) |  you're using a certain query for just

one day, and you're hitting 5,000 rows limit. I don't know. I haven't tested it that's still the case or not. JOHN MUELLER: OK. MIHAI APERGHIS: But other than that, you're-- the more specific you go, the more you run into certain limitations to protect user privacy and limit the amount of information you might be able to get. JOHN MUELLER: OK, that sounds in line, what you're seeing there,  

![](https://i.ytimg.com/vi/u7JY_Tz2c4U/hq3.jpg)



#### [0:40:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2430) |  [? Kazushi, ?] in that case. MIHAI

APERGHIS: If people can use the community forums to post API queries with more information, I can replicate them and see if there's anything that-- maybe some discrepancies with a UI, or anything like that. JOHN MUELLER: OK, cool. Sounds like something you do. Cool, all right. Infinite scroll-- do the old recommendations still  

#### [0:41:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2460) |  play a role? Yes, they continue to

work, surprisingly. The only thing in those recommendations that you can ignore nowadays is the rel-next and rel-previous, of course. But otherwise they continue to work. So in particular, having the paginated links on the bottom, that helps. Having URLs that update as you scroll through, that helps us.  

#### [0:41:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2490) |  So those things continue to be pretty

good. I'm kind of torn about updating the code to remove rel-next and rel-previous, because we have it in the blog post as well. But maybe it makes sense to do a new blog post and then updated the code to match the current state of things. MIHAI APERGHIS: John, can have a quick follow-up on that? JOHN MUELLER: Sure. MIHAI APERGHIS: So I worked with a website where they weren't able to implement the recommendations  

#### [0:42:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2520) |  for infinite scrolling exactly as they were

in the documentation due to some technical limitations. So what they did instead is they were able to implement a standard navigation like page equals 2, page equals 3, and so on. But they hid it. And so it's a display none, or something like that. And would Google still be able to use that? Because-- so that users can use the infinite scrolling, but that doesn't really update the URL or does anything.  

#### [0:42:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2550) |  And Google used the actual page equals

something URL, even though they're hidden from the users. JOHN MUELLER: That would probably work. I don't think, from a usability point of view, it's perfect, but it would probably just work. MIHAI APERGHIS: OK, that's cool. JOHN MUELLER: All right, question about image URLs when internationalizing content.  

#### [0:43:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2580) |  When there is an image URL embedded

in a properly hreflang'ed international URL-- say Japanese and English-- I'm thinking Google Image Indexing will be able to annotate the image with both languages, enabling the image to appear in the search results for both language locale pairs. Is that correct? Yes. So at least as far as I know from all of the things that I've seen. So one of the things that happens here is we will see that this image is  

#### [0:43:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2610) |  being used on multiple pages, and we

will try to match the best landing page for the query. So it's not so much a matter of you're doing this fancy hreflang linking between different pages, but more like we notice this is the same image. So we can pick that up for image indexing ones. And we have multiple language pages that are associated with this image, so for specific queries, we'll try to find the best landing page for that query. And if someone is searching in Japanese, then, obviously, we can match the Japanese query  

#### [0:44:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2640) |  with Japanese text on a page. And

similarly for English and English. I suspect it's trickier when it comes to different countries and the same language. I don't know if we would use hreflang in Google Images. So that's something where I'm not 100% sure about. But definitely, when it's different language content, then that's kind of easy for us to match this text goes to this page, and this text goes to the other page.  

#### [0:44:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2670) |  "How do you deal with a site

owner that creates a fake persona for their site? So apparently a competitor is doing this, and they're wondering, is this against Google's guidelines? Would this lead to a manual penalty? Curious to know what mechanisms you have to catch those out. Is there anything that we should or could do about this?"  

#### [0:45:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2700) |  So from my point of view, or

at least from what I've seen with regards to the webmaster guidelines, I don't think this would be against our webmaster guidelines. But this would be something that we'd probably just pick up indirectly and try to figure out in other ways, in that we'd recognize, well, maybe this site is not the best result for these kind of queries. And it's not so much that we'd have algorithms that it would say, oh, the person that is specified  

#### [0:45:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2730) |  on this page does not exist, or

is someone else, but more like, well, in general, we don't think this is a good result for these queries. So it's not that you could go to the Web Spam Forum and report that and say, this person is wrong. It's more something that we would try to pick up indirectly with regards to our quality algorithms overall. Woohoo, I think we made it through the questions, and so--  

#### [0:46:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2760) |  And we have 10 minutes' time. Wow,

how do we do that? I'm sure a bunch of new questions are added as well. Let me see. Oh, just three. "We use a search console API. Have been noticing a loss of data and UI discrepancies. Is there a timeline or any information about this data issue? Is there a way to get that data back?" I'm not sure which parts you're referring to.  

#### [0:46:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2790) |  In general, the API should match what

we show in the UI. We did have some indexing issues, I think, in April, and for a period of I don't know, two weeks, maybe? Not sure. "We don't have data for the index coverage report, and that's visible in the UI as well. And that's something that would be visible in the API as well. But otherwise, it should be the case that there would be significant differences  

#### [0:47:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2820) |  with the UI or the API if

they both used the same underlying data." It might make sense, like Mihai mentioned, to post some of the details of what you're looking at in the webmaster health forum so that others can take a look there and escalate that if necessary. JAY: "[INAUDIBLE] I'll be doing. It's about two weeks of data in March 3. Currently the 17th or 18th are sort of missing. And otherwise, the rest of the API seems to be bringing the data exactly as we would expect.  

#### [0:47:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2850) |  So it's sort of a bizarre-- wondering

if it's an upstream issue. Some sort of little bump in the road. JOHN MUELLER: Which data are you looking at there? Is that the performance? JAY: Performance, keywords, pages, country. It's basically like every field seems to just be done, right there. And as best as we can tell the, UCL is set up completely correctly to pull that data. So it's a little bizarre. JOHN MUELLER: OK, I don't know. It would it be useful to have the details there.  

#### [0:48:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2880) |  What I've also sometimes seen is that

we switch the canonical URL to a different site sometimes. So for example, it might switch between the dot-dot-dot and the non-dot-dot-dot version. And depending on how you're pulling that data, you might be pulling the dot-dot-dot version of the data out. And for a certain period of time, we switched to the other version and tracked the data there, then that wouldn't be visible in the version  

#### [0:48:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2910) |  that you're tracking. But that's something that

should be pretty rare and should be visible in the UI as well. JAY: OK, thanks. I'll be following up with Mihai about this one. JOHN MUELLER: Cool, all right. Does Google punish sites if they find your political preferences, based off of your Gmail or search history, is not the same as Google employees? No, no.  

#### [0:49:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2940) |  I don't think that would ever make

sense. So I mean, we don't use things like Gmail when it comes to search anyway. But essentially, there are sites out there for all kinds of weird things. And that's what makes the web unique and valuable, in that people can just publish content, And it's out there. And you can write about what you want, what you believe. And I think that makes search like a magical place, in that you can find all of these things if you look for them,  

#### [0:49:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=2970) |  and you can publish them like that.

There is like nothing where we would manually say, we will I don't know place a manual action on a site that we don't like. All right. What else is on your mind? Wow, man, don't tell me we have everything answered? I can cancel the rest of the session.  

#### [0:50:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3000) |  I did notice like YouTube just sent

me an alert before when I started the Hangout that these Hangouts are going away, apparently. So I need to figure out some other ways to do these office hours hangouts, because I find them really useful. So we will have to figure something out. The setup might be a little bit different. Hopefully, it doesn't get too complicated. But we'll figure something out. SPEAKER 1: OK, all right. Now, I've got the question related  

#### [0:50:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3030) |  to the first one about the [INAUDIBLE]

by same content. Well, not the same content; the same popular use. Different angle, then I publish, obviously, in a sim domain. What if I separate into the subdomain, and I brought up abc.com, articles with different angle? And there, probably, we will realize show the interior results. JOHN MUELLER: That's fine, yeah. SPEAKER 1: [INAUDIBLE] speaking, I'm just putting in the same domain [INAUDIBLE]?  

#### [0:51:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3060) |  JOHN MUELLER: With regards to the number

of results that we show from the same site, it's a bit tricky. Because sometimes, we see subdomains as a part of the same site, and sometimes, we see them as different sites. So that's something that our algorithms try to figure out algorithmically. For example, if you look at a shared hosting provider, like Blogger, for example, all of the subdomains are completely separate sites. And sometimes if you look at a site where they'll use subdomains for different themes  

#### [0:51:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3090) |  or different parts of the same site,

that's why we would recognize the whole domain is one site. So it's not that straightforward to say, if it's a subdomain, we'll treat it like this, and if it's in a subdirectory, we'll treat it like this. And I think that's also where the whole kind of discussion about subdomains versus directories goes, in that sometimes, we treat it as one type. Sometimes, we treat it as different types. Both of those situations can happen. SPEAKER 1: Yeah, I mean, this kind of discussion  

#### [0:52:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3120) |  will be able to pop out again

[INAUDIBLE].. Isn't it [INAUDIBLE]? JOHN MUELLER: Oh my god. SPEAKER 1: Let's me just put-- reading your question, it's like if I have to the same domain and I won't use to separate into two subdomains, I think, well, the subname will be the Google [INAUDIBLE],, and a brand new domain, right? Authority and blah-blah-blah then we'll be like counting as a brand new. And so the original article then moved to there with 301. Will the ranking drop? We worried we'll pass over it 100% of the signal there.  

#### [0:52:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3150) |  JOHN MUELLER: It's hard to say. I

think offhand, if it's a new subdomain, and you haven't you subdomains before. Then we will probably treat that as a separate site. SPEAKER 1: All right. JOHN MUELLER: It will would be such that the same content, when moved to a subdomain on its own, will probably struggle a little bit more than if you had it within the same content. So that's kind of like you're taking one small piece out and putting it on an island of its own.  

#### [0:53:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3180) |  Then back, that island content has a

little bit harder than if it's seen as a part of this big picture that you already have created, SPEAKER 1: All right. I was just making sure I asked, to see how it goes. JOHN MUELLER: Of course, Yeah. I mean, these things are easy to try out. And the only thing that I would caution against is sometimes, when you set up test sites for these kind of things, a test site is a bit of an artificial construct, in that you might see one result with a test site that doesn't match what  

#### [0:53:30](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3210) |  would happen with a normal website when

you do something similar. So that's one thing to keep in mind. SPEAKER 1: Cool. JOHN MUELLER: Cool, OK. So I need to head out a little bit earlier as well. So it's kind of cool that we made it all the way through. We have the next Hangout in English lined up for Friday, and a German one on Thursday. And if any of you have ideas on how to run these office hours  

#### [0:54:00](https://www.youtube.com/watch?v=u7JY_Tz2c4U&t=3240) |  hangouts outside of this YouTube setup, then

send them my way. I'd probably end up trying different things until we figure out exactly how it works. But thanks for joining in. Thanks for all of the comments and questions that were submitted. I hope this was useful, and see you one of the next times. THOMAS: All right, John. Thank you very much. SPEAKER 1: Thank you. Bye-bye. JOHN MUELLER: Bye. MIHAI APERGHIS: Bye, John.  