[![English Google Webmaster Central office-hours from Sept 20, 2019](https://i.ytimg.com/vi/kPVNl7btFHQ/hqdefault.jpg)](https://www.youtube.com/watch?v=kPVNl7btFHQ)

## English Google Webmaster Central office-hours from Sept 20, 2019

This is a recording of the Google Webmaster Central office-hours hangout from Sept 20, 2019. These sessions are open to anything webmaster related like crawling, indexing, mobile sites, internationalization, duplicate content, Sitemaps, Search Console, pagination, duplicate content, multi-lingual/multi-regional sites, etc. 



Watch out for new sessions, and add your questions at https://www.youtube.com/user/GoogleWebmasterHelp/community



Feel free to join us - we welcome webmasters of all levels!



#### [0:00:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=0) |  JOHN MUELLER: All right. Welcome, everyone, to

today's Webmaster Central Office Hours Hangouts. My name is John Mueller. I am a webmaster trends analyst here at Google in Switzerland. And part of what we do are these hour Hangouts, where webmasters and publishers can pop in and ask us questions around their websites and web search, anything like that. A bunch of questions were submitted on YouTube already, so we can go through some of those.  

#### [0:00:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=30) |  But if any of you have anything

on your mind that you'd like to talk about beforehand, feel free to jump in. SAIDUL HOQUE: Hi, John. JOHN MUELLER: Hi. SAIDUL HOQUE: I would like to ask some question. And so one of our clients, they are a nonprofit organization, so they arrange a lot of events, a lot of times. So when they're adding an event, they  

#### [0:01:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=60) |  publish it on their blog post, on

their blog, about the event. But when the event is finished, they take off the page from their blog. So in this case, do we need to create an [INAUDIBLE] for those event page? JOHN MUELLER: I think the part that's kind of-- where they'd be missing something where it would be useful for their website in general  

#### [0:01:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=90) |  is if there were some kind of

persistent landing page for these kind of events. So for example, if they do a monthly event, then having one page for that monthly event, just kind of as a placeholder-- we do this event every month-- and some information about it, so that people can link to some persistent place and that page can gain value over time. Whereas if you have just individual event pages for each date, and those turn into 404,  

#### [0:02:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=120) |  then that's something where all of the

links, all of the value that they collect for those pages, they kind of disappears again. So something kind of like a persistent page that says, our next event is then. And if you want to keep an archive of the old events, maybe have a separate archive section, but kind of have one persistent place for the events. SAIDUL HOQUE: OK. Do you suggest the same thing for the offer or special promotion?  

#### [0:02:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=150) |  [INAUDIBLE] JOHN MUELLER: I mean, it depends

on how you want to have those found. So that's something where if you do this really as a complete one-off thing, where you don't care if people, I don't know, link to it over time because it's such a one-off thing, then that's something that you might just want to keep and remove. But if it's something that happens regularly,  

#### [0:03:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=180) |  then having a persistent page is fine.

So if you have something like, I don't know, Black Friday offers, and you have a page that you always use for Black Friday, then that's something that might make sense. Whereas if you have just, like, one page that says "Black Friday 2017," "Black Friday 2018," then all of those pages will kind of have to stand on their own, and it'll be hard for a new page to gain value there.  

#### [0:03:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=210) |  SAIDUL HOQUE: The last question. We have

a client. They they build garden sheds and at the same time, they build playground. So they have two types of product. So what they have decided to submit, they build a new website for the playground product. And they want to move all the products from the website to the new playground website. So they want to redirect all the playground products to the new domain. The question is, they're going to have, on the playground with the keywords, the old domain,  

#### [0:04:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=240) |  will it transfer to the new domain

with this [INAUDIBLE] redirection, or will it take time? JOHN MUELLER: If they're creating a new website that is a mix of multiple other websites, then that's something that is not the same as just moving from one domain to another. So if, for example, they have all of their content on one domain and they just want to move to another one, that's something where a site move, a setup with a redirect works really well.  

#### [0:04:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=270) |  But on the other hand, if they

have playgrounds and garden furniture, and they want to create one new website that is a mix of both, or they want to take one mixed website and split it into two separate websites, then that's something where we essentially have to re-evaluate the final state. We can't just say, like, this plus this equals the new one. We have to figure out what the right balance is. SAIDUL HOQUE: Thank you, John. JOHN MUELLER: Sure.  

#### [0:05:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=300) |  Any other questions before we jump in

with the rest? That's fine, too. I mean, you're welcome to jump in in between if anything pops up, or if there's anything that I can help clarify along the way. Let's see. What do we have? Our news article template features two bylines-- one at the top below the title, which includes only the author  

#### [0:05:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=330) |  name, and another one on the bottom,

which includes more detailed information about the author. Is it OK to have two bylines on the page? Does it create any confusion to the crawler in understanding who the author is? So from our point of view, it's fine to have these bylines on the page. It's not something where we're explicitly looking for that. We don't use the authorship markup or anything specific like that. It's essentially just a way for you  

#### [0:06:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=360) |  to tell your users what this page--

kind of the background of that page. Like, if you're just saying, this is written by these authors, then that's a good place to put it. Some people put it on the top, some people put it at the bottom, some people have both. I think that's totally up to you. What's the best place for the schema markup with item prop author? Shall we add it to both bylines, or just to one? So from my point of view, where you add  

#### [0:06:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=390) |  it is essentially up to you. What

I would not recommend doing is adding it in two places on the same page, because that just makes it a lot harder to maintain properly and to understand what it is that you're trying to tell with the markup. So from that point of view, I would pick one place and use the markup there and not just put it in two places. Or if you use JSON-LD for the structure data, then you could put that somewhere in the head,  

#### [0:07:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=420) |  for example. It doesn't need to be

exactly in the same place where the name, for example, would be. We're an online tour operator and have implemented aggregate rating for offered accommodations via a schema at org/hotels and our offer destinations via a schema.org/webpage. And I think it goes into the change  

#### [0:07:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=450) |  that we recently had with regards to

the review rich snippets that we would show in search, where we essentially made three changes. On the one hand, we limited the review rich snippets to a certain set of types, and on the other hand, we made it so that if you're reviewing your own entity or your own business, then you can't  

#### [0:08:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=480) |  host that on your own site, or

rather if you host it on your own site, we just wouldn't show that. So those are kind of the main changes that we did there, and I think what you're probably running into is you used to review essentially the web page. And that's not something that we support anymore. So from that point of view, that setup that you have where users can review the page  

#### [0:08:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=510) |  is not something that, from our point

of view, makes a lot of sense. We expect that the reviews are about a specific product or a specific thing that is actually being reviewed, something that anyone can look at and review in a comparable way. So for example, if you, I don't know, have a car that people are able to review, then everyone is able to buy that car and able to review exactly the same product.  

#### [0:09:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=540) |  Whereas if you have something like the

destination page that you mentioned there, I think it would be hard for multiple people to review the same region of, like, geographic region that someone could go to and review based on the same criteria. So probably that wouldn't make that much sense in there. Of course, it's totally fine to show these kind of reviews on your pages  

#### [0:09:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=570) |  if you think that it makes sense

for users. It's just something that we wouldn't show in the search results. Our site got hacked. The hack got through-- oh, is there a question? Sorry. SPEAKER 1: Hey, John. JOHN MUELLER: Hi. SPEAKER 1: My question has to do with Google Indexing API. JOHN MUELLER: OK. SPEAKER 1: Yeah. I'm-- they said you started with Indexing API for one of my blog sites or job sites.  

#### [0:10:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=600) |  But there's an issue with the get

metadata using the-- with the API. And I'm not sure as to what API key I'm supposed to use. During the app installation, I did create the service account [INAUDIBLE] in all the three authorizations that I have gone through. So but still, I am not able to figure out what API key I'm supposed to use in that. And also, it got in the patch prime of the process in the patch. JOHN MUELLER: Now, I would check in with one of the Google API's  

#### [0:10:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=630) |  help forums, because that's something which is

kind of hard to explain just live in a Hangout. It's almost helpful more to kind of look at the code that you're actually trying to use. So for the-- SPEAKER 1: Ah. JOHN MUELLER: Yeah, go ahead. SPEAKER 1: Yeah, I did post in the forum, but there is no specific forum for Google Indexing API.  

#### [0:11:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=660) |  So it's-- whatever, even if I search,

[INAUDIBLE] webmaster API. And if I posted there, I think that [INAUDIBLE] it was flagged as out of topic in that. So it's kind of [INAUDIBLE] circulate and post in the room. JOHN MUELLER: Another idea might be to just post on Stack Overflow. Since this is kind of a generic Google API, that seems like something that someone from Stack Overflow would also probably be able to help with.  

#### [0:11:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=690) |  SPEAKER 1: OK. JOHN MUELLER: OK. AMOS

GRIMA: John. John, hi. JOHN MUELLER: Hi. AMOS GRIMA: Amos here. In the last Hangouts, I was asking about the Google for Jobs indexation, and we are having problems here with our visibility. We have begun a migration, and we saw a drop-off in basically our Google for Jobs traffic. And I was wondering-- you said ping you, the website,  

#### [0:12:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=720) |  et cetera. I did that in the

comments. I was wondering if you had any updates. JOHN MUELLER: I don't think I heard back from the team, but I pass it on to them to look at. But it might be good to check in with them again if you aren't seeing any changes. Usually they're pretty good at picking these kind of things up, but if you're still seeing problems in that it's not performing the way that you expect,  

#### [0:12:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=750) |  then I am happy to update that

again. If you can just send me a quick note, then I won't forget. AMOS GRIMA: OK. I'll add this in the comments. JOHN MUELLER: Sure. AMOS GRIMA: Thank you. JOHN MUELLER: Thank you. Awesome. OK. Our site got hacked. The hack got removed, but it left virtual pages indexed in Google. How can I de-index those pages? So sorry to hear that your site got hacked. That's something that happens to lots of sites, so it's always a bit of a hassle.  

#### [0:13:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=780) |  With regards to the hack got removed

and some virtual pages are still indexed in Google, the first thing that I would double-check is that these pages are actually no longer existing on your website specifically for Googlebot. So what we sometimes see is that hackers create this kind of virtual pages or fake pages on a website that, when you try to go to them manually, they don't work  

#### [0:13:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=810) |  or they look like they don't work,

and when you go there with Googlebot to look at the page itself, then they do show some content. So for a webmaster, it looks like it's completely cleaned up. And for Googlebot, we look at it and say, well, all of this content is still here. We'll continue to index it. So that's kind of the first step that I would do-- check with the testing tools to make sure that it's really, really removed. And then the second step, with regards  

![](https://i.ytimg.com/vi/kPVNl7btFHQ/hq1.jpg)



#### [0:14:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=840) |  to getting these out of Google, there

are essentially two approaches that you can take. One is if you have individual pages that are very visible in your normal search results, then that's something you could remove with the URL removal tools in Search Console. So with those, you can specify a subdirectory. If these are tied to a specific subdirectory-- often they are-- then you can specify a subdirectory and say all of the things in this subdirectory I don't  

#### [0:14:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=870) |  want to have shown in search. And

then for a certain time, I think it's something like 180 days, we will filter all of those out from the search results. For individual URLs, if you can't isolate a subdirectory, then you can submit them there as well. It's just a very manual process, and there are some limits. I think it's something like 1,000 URLs a day that you can submit there. So because of those limits, I would focus on the URLs that are actually  

#### [0:15:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=900) |  visible for your normal search results. So

if you search for your company name and you find five hacked pages from your website, that's something definitely to clean up. On the other hand, if you search for specifically that hacked content, where you search for your company name and then "free download" and whatever hack content they had on your pages, and then you find those hacked pages. And probably that's not as critical, because normal users wouldn't be searching like that.  

#### [0:15:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=930) |  They'd be searching for your company name

or for your products. So from that point of view, usually there's a set of very visible pages that you can remove, and those are the ones that I would remove. For pretty much everything else, I would recommend just making sure that those pages return a normal 404 status code. That tells us this page no longer exists, and the next time we go off and try to crawl those pages, we'll see these are gone and we can just drop them  

#### [0:16:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=960) |  from the search results. So that means

there's no extra work involved for you. It's kind of cleaned up automatically over time. It can take a bit of time, so it's-- often it'll take a couple of months, maybe half a year, for all of these pages to be updated and dropped out. So it's not something that happens right away, but again, unless you're explicitly looking for the hacked content, oftentimes you don't see those pages. So the-- kind of add two approaches  

#### [0:16:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=990) |  I would take, and definitely make sure

before you go down that path, that these pages really don't exist anymore for Google. And if they still exist for Google, then obviously there is still something on your site that's kind of affected from this old hack. How do crawlers deal with GDPR cookie bars? Obviously, no content may be loaded before a user has set an opt-in. We see many datas which are not loaded in Search Console,  

#### [0:17:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1020) |  and they seem blocked. Yes, that can

be problematic. So in general, Googlebot does not click through on anything. So Googlebot will not go to an interstitial and say, I accept. Instead, Googlebot will try to crawl the page as it comes to Googlebot and it will try to render the page as it comes to Googlebot. So in particular, if you have a kind of a banner that  

#### [0:17:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1050) |  is shown on part of the page,

or if you have a banner that is shown on top of the actual HTML content, then that's something where we can still see the actual content. We can still index the actual content. So that's less of an issue. On the other hand, if no content at all is loaded, and it's just this interstitial page, then we would only see the interstitial page and think, well, this is what you want to have indexed,  

#### [0:18:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1080) |  so we'll try to index it for

you. And that's probably not really what you want to have indexed. The other variation that I sometimes see is that you redirect to an interstitial URL and then from there, when the user has accepted, you redirect back. That's equally problematic because the content, on the one hand, is not there when we look at that interstitial URL. And oftentimes, the URL of the page  

#### [0:18:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1110) |  itself is included in a kind of,

I don't know, a cookie or some kind of a snippet that we can't pick up. And what happens then is we see all of the pages on your website redirect to one page and we think, well, you're redirecting your whole website to one single page, and this single page has some legal information on it. So maybe you want to replace your whole website with this one page. And probably, you don't want to do that.  

#### [0:19:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1140) |  So from that point of view, I

would recommend against doing this kind of redirect thing, instead showing a banner on top of the actual content, so that users get the banner that you need to have shown for legal reasons, but that the actual content is still loaded behind so that search engines can actually process that. Of course, the other thing that you might be able to do is to think about which users actually  

#### [0:19:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1170) |  need to have this interstitial shown. And

depending on what it is that you need to do, it might be that you discover that perhaps users in the US don't need to have this interstitial shown. I don't know the guidelines in this specific case, but that's something that you can double-check. Googlebot primarily crawls from the US, so if there's something that you don't need to show users in the US, then you don't need to show that to Googlebot either.  

#### [0:20:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1200) |  So that might make it a little

bit easier. In some legal cases, it can make it a little bit harder where, if there are policy reasons or legal reasons why you can't show any of your content in the US, then obviously Googlebot can't see that content, either. So that's kind of depending on which case applies to your specific website, there are different options there. The way to kind of test this is, ideally,  

#### [0:20:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1230) |  to use the Inspect URL tool, where

you can see how we would render a page. And you can also look at the HTML that we pick up and use for indexing. So if you check your pages and you see they-- and the interstitial is being shown in the rendered version, as long as within the HTML version, the actual content is still shown, that's kind of OK, because then we can still pick up the actual content.  

#### [0:21:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1260) |  On the other hand, if you look

at the HTML version and there is no content at all, other than the interstitial, then essentially, we only have the interstitial content to index, and that's probably not what you want. NANDHINI BABU: Hey John, I have a question. This is related to the image sitemap. We have hosted our images in Amazon AWS, so the URL comes as the S3 bucket, then as a subdirectory, we have our domain name. So can I just use these URLs in our image sitemap?  

#### [0:21:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1290) |  JOHN MUELLER: Sure. Sure. That's perfectly fine.

Images don't need to be on the same domain name. So that's-- like, if you're using a CDN like Amazon, if you're using any other kind of CDN, you're welcome to host your images there. The one thing I would kind of caution about, though, is indexing images takes a lot longer than indexing web  

#### [0:22:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1320) |  pages. [WHIRRING NOISE] So if-- whoops, little

bit of noise. Hang on. Da-da-da. So it-- the thing with images is images take a lot longer to be indexed, and they take a lot longer for redirects to be processed. So if at any point in the future, you decide to move to a different CDN setup, you would have to set up redirects from the old URLs to the new ones. And for images, processing those takes a lot longer.  

#### [0:22:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1350) |  So if your website really depends on

images, on image search in particular, then that's something where I would recommend going down the path of using a subdomain from your own domain, which you can then kind of route independently depending on the CDN that you're using. So if you use a subdomain, then those image URLs can remain the same for a longer period of time,  

#### [0:23:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1380) |  and you can swap out the CDN

in the back without as much problems. I don't know if that's possible with kind of the AWS S3 bucket setup, but that's kind of what I would recommend doing with images, especially if you're unsure of the long-term setup that you have with your infrastructure, which CDN do you want to use-- like, which one is cheaper, which one is faster,  

#### [0:23:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1410) |  those kind of things. So the closer

you have your URLs on your own domain, the more likely you'll be able to just reuse those URLs for the long run, and that makes it a lot easier for when you have to change the infrastructure. NANDHINI BABU: OK, thanks. JOHN MUELLER: Sure. JESSY SEO: Hi, John. I got a question about the new rel link for linking. There's a-- I don't know if now it's a [INAUDIBLE] Google  

#### [0:24:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1440) |  respect the new follow directive, but when

you are on a commercial website, you have a link for categories and a link for attributes. Don't know the name in English. Facet, navigation-- JOHN MUELLER: Facets, yes.  

#### [0:24:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1470) |  JESSY SEO: --with attribute. and in new

commerce, for example, for each attribute, there's a link to a page for whatever-- new-- no interesting content for Google, but is useful for the user. And sometimes, I just put nofollow to this sort of link  

#### [0:25:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1500) |  so Google engine now didn't crawl this

link. And now with this change, what Google will do? JOHN MUELLER: So it's not 100% defined, but the plan is to make it so that you don't have to make any changes. So we will continue to use these internal nofollow  

#### [0:25:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1530) |  links as a sign that you're telling

us, these pages are not as interesting. Google doesn't need to crawl them. They don't need to be used for ranking, for indexing. So it's not a 100% directive like robots.txt, where you say these are never going to be crawled, but it does tell us that we don't need to focus on them as much. So for us, the main change with nofollow and these new attributes is for outbound links,  

#### [0:26:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1560) |  so from your website to another website.

JESSY SEO: Thank you. JOHN MUELLER: Within the website-- yeah, for these kind of faceted navigation, for categories and sorting and things like that, that continues to work. JESSY SEO: OK. And for those attributes you usually see and sponsored services, what is a way, as a developer,  

#### [0:26:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1590) |  we must implement it? JOHN MUELLER: You

don't have to implement it. We think it's a good idea to make it easier for search engines to understand-- [INTERPOSING VOICES] JOHN MUELLER: --which of these are-- sorry? JESSY SEO: There's a rule for this to add for Google to understand more of the content,  

#### [0:27:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1620) |  or it should just [INAUDIBLE] for Google

and there's no incidents on the ranking, on the page rank, [INAUDIBLE] over a website? JOHN MUELLER: Yeah, it's essentially the same handling as with nofollow, except that you're telling us a little bit more detail of why you want to nofollow this link. So is this an advertisement, or is this blog comments?  

#### [0:27:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1650) |  And so those kind of differences can

make it a little bit easier for our algorithms afterwards to think about, well, maybe these are good blog comments, for example, and maybe we should focus on them a little bit more. But at least we have the ability to understand the difference of what you're trying to say. With the nofollow in the past, you're basically saying, look, I just don't want you to look at this link. And with the new attributes, you can tell us a little bit more  

#### [0:28:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1680) |  why you don't want us to look

at it. And again, it's for outbound links, so it's not something that, like, within a website you really need to worry about. JESSY SEO: OK. And there's no way to use an HTML sync to have a sidebar with a [INAUDIBLE] section for comments and to be better understand by Google?  

![](https://i.ytimg.com/vi/kPVNl7btFHQ/hq2.jpg)



#### [0:28:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1710) |  So when you have a sidebar or

[INAUDIBLE] comments and all in this blog is a nofollow link, you usually see a link? JOHN MUELLER: So-- JESSY SEO: So you have HTML structure of a page with different-- because you have a section with a site for all link.  

#### [0:29:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1740) |  We have no impact with the content?

JOHN MUELLER: OK, so something where you say, everything in this section should be no followed, or-- JESSY SEO: Yes. JOHN MUELLER: Yeah. I don't think there are any plans to have kind of a section or an HTML element level nofollow. So you would either have to do that on the page level, which usually doesn't make much sense, or on the per link level, so that you mark up  

#### [0:29:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1770) |  each individual link. JESSY SEO: OK. Thank

you. JOHN MUELLER: Sure. MOSHE MA-YAFIT: Hi, John. JOHN MUELLER: Hi. MOSHE MA-YAFIT: So two questions, please, if I may. JOHN MUELLER: Sure. MOSHE MA-YAFIT: First question will be, so few days ago, three of my website, which are all domains, were rolled to mobile first index. And I was wondering if you just ran another batch of indexing  

#### [0:30:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1800) |  for mobile first index, or this is

like-- have you heard from other webmasters lately being rolled to mobile first index, or this is just my case? JOHN MUELLER: I don't think they picked just your websites. So this is something that we want to roll out to all websites. So we're doing that step by step. As we can figure out that sites are more or less ready, we shift them over to the mobile first indexing.  

#### [0:30:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1830) |  So that's-- the web is pretty big,

and I think we have half of the search results moved to mobile first indexing now, probably a little bit more. So the team is still moving forward on that. MOSHE MA-YAFIT: Great, thanks for confirming. Second question-- I don't know if it relates to this discussion, but I've seen the video Bartosz did with you and Martin, which was just blow  

#### [0:31:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1860) |  minding, hearing those things. And my question

was, so if I have a full JavaScript website, single page application, should I expect Google to be able to crawl-- to fully crawl my website in the future, also taking into consideration that lately, Googlebot move to the evergreen version,  

#### [0:31:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1890) |  so should we expect in the next

few months to have Google able to crawl the website fully, also the JavaScript in a manner of sense? JOHN MUELLER: So we're getting better. So it's-- from that point of view, I don't think we can say we will absolutely be able to index everything with JavaScript. And we don't have any specific timeline, or we say by then we will be able to do that.  

#### [0:32:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1920) |  I think there will always be specific

edge cases that make it hard sometimes, so a really basic one is a lot of JavaScript-based web apps use one single URL for all of their content. Like, you click around, and it stays one single URL. And if you just use one single URL, then we will be able to only index that one single URL and we won't see all of the other content that you could load if you click on different things.  

#### [0:32:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1950) |  So that's one kind of basic example,

where we can do a lot of things with JavaScript, but we can't make up URLs for your website that don't exist and index content like that. So from that point of view, I think we can get a lot better. We can definitely get into a situation where a normal JavaScript-based website that uses different URLs and uses normal links, then that we  

#### [0:33:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=1980) |  should be able to crawl and render

that and index it normally. I kind of see that as a reasonable expectation for the team. I don't think that will happen in the next few months, but we're working on it. But we definitely won't be able to just take any kind of JavaScript application or website and automatically make sure it will be completely workable in Search. So that's one thing there.  

#### [0:33:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2010) |  Let's see, something else I wanted to

kind of add there. Oh yeah, of course, the other thing is if you currently have a JavaScript-based website like that, you probably don't want to be in a situation where you're waiting for Google to figure it out. So if you're currently thinking about Search and you already have a JavaScript-based website, then there are ways that you can make sure that it works, where you don't have to wait.  

#### [0:34:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2040) |  So if you have a business, if

you have something that is important to be found in Search, then don't just kind of say, well, Google will figure it out someday. Instead, just kind of follow the guidelines that Martin has been putting together, the JavaScript videos that he's been putting up, and make sure that your site works in Search, because it's possible to make JavaScript-based sites work well in search. And it's also possible to make them work well in other search engines.  

#### [0:34:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2070) |  So Google is able to do a

lot with JavaScript sites, but not all other services and search engines are, and there are ways that you can kind of work around that. MOSHE MA-YAFIT: Yes-- JOHN MUELLER: So those are-- yeah. MOSHE MA-YAFIT: If I may share, so the technical solution we have is dynamic rendering. JOHN MUELLER: Yup. MOSHE MA-YAFIT: And we built it with own internal coil. And internal coil goes on the site.xml and creates snapshots for all the URLs it finds in the sitemap, and then saves  

#### [0:35:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2100) |  those pre-rendered HTML version in the server

to serve Google this version. But I do remember in the previous I/O, you mentioned that hybrid rendering will be the future. So I wonder if it's still the case, because we haven't been able to improve our rendering solutions like moving from dynamic rendering to either server side rendering or hybrid rendering.  

#### [0:35:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2130) |  Is this still the recommended solution? JOHN

MUELLER: I think you can do it with dynamic rendering or the setup that you have now. So there are different ways that you can do it, and they also have different effects on search engines as well as on users kind of from a speed point of view. So I wouldn't just blindly switch from one technology to another just because you  

#### [0:36:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2160) |  think it might make sense. But I

would test it up and make sure that it works well for your users from a speed point of view. Sometimes with server side rendering, you can do really fancy things from speed, and similarly with dynamic rendering, it's something that works a little bit different. But there are trade-offs in the different approaches. So I would try it out with something small to see what are the effects on your specific use  

#### [0:36:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2190) |  case, and does it make sense. MOSHE

MA-YAFIT: OK, thank you very much, John. It was very helpful. I appreciate it. JOHN MUELLER: Sure. CHRISTIAN NEUWERTH: Hey John, sorry for interrupting again. JOHN MUELLER: No problem. CHRISTIAN NEUWERTH: So first of all, I have to say you-- I hope there will be more Hangouts in the future so we have a little bit more time to get all the questions answered. You're doing a great job. So-- JOHN MUELLER: Thank you. CHRISTIAN NEUWERTH: The question I have is a follow-up to the schema part,  

#### [0:37:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2220) |  and would be the Google notice making

review rich results more helpful. To make it short, maybe you can make a yes or no. Will this list be extended in the future, or is it final? JOHN MUELLER: I don't know. I would assume it's final for the moment, because these kind of changes get discussed a really long time internally. And it's not something where they will kind of like say, oh,  

#### [0:37:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2250) |  we will just make a small list

and then change it a few days later. They kind of want to have a stable state. CHRISTIAN NEUWERTH: Yeah, but why I'm asking-- earlier we, in a Hangout last week, we were just asking if we may change the schema type from product to hotel for reviewing products, in a way. And yeah, actually we are worried about investing more in schema in general.  

#### [0:38:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2280) |  So can we make a change here?

Should we make a change? And the question about reviewing the regions or the destinations which has been asked earlier in this Hangout-- we see that Google collects stars for skiing areas. In the Knowledge panel, for example, you can see the stars are there. So we think we are collecting them in the right way, and now I'm wondering if we can maybe implement a ski resort  

#### [0:38:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2310) |  or something like that, because yeah, we're

not sure if this will be for the future. JOHN MUELLER: Yeah. [INTERPOSING VOICES] CHRISTIAN NEUWERTH: So. JOHN MUELLER: Yeah, so one of the things with regards to the blog post is that I think the types that we listed include their respective subtypes. So something like organization, I imagine, includes things like hotels. And maybe it even includes things  

#### [0:39:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2340) |  like ski resorts, which are kind of

like one clean thing, where it's run by one organization. I don't know for sure. So I would double-check that. But in the schema.org documentation, you can see kind of the higher level types and the subtypes, and that's something that might make it possible for you to kind of pick a type that matches both what you're trying to do and that aligns with what we're trying to do.  

#### [0:39:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2370) |  CHRISTIAN NEUWERTH: All right. Yeah, cool. JOHN

MUELLER: Cool. You also had, I think, a question about the spam network. I think it was from you. CHRISTIAN NEUWERTH: Right, yeah, exactly. JOHN MUELLER: OK. Can you tell me a little bit more about what you're seeing there? CHRISTIAN NEUWERTH: Yeah. Each day, we are seeing links that pop up from that network. So a lot of domains appear, and yeah, we're wondering-- do they hurt? Don't they hurt? And we are uploading the disavow file, not on a daily basis,  

#### [0:40:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2400) |  of course, but sometimes for things, we

have to. So-- and we have to do it for a lot of properties, and that makes it even more hard. So yeah. And we have an agency that says the top two rankings, or two agencies, which we're doing an audit. So everyone is thinking we should re-upload the disavow file, but I can't do it on a daily basis. JOHN MUELLER: Yeah. CHRISTIAN NEUWERTH: I don't want to do it.  

#### [0:40:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2430) |  JOHN MUELLER: So yeah, usually with these

kind of things, that's something that we can catch fairly well on our side. So you don't have to do that on a daily basis-- definitely not on a daily basis. So that's not something that should be affecting your site negatively. It sounds like you're really worried about this particular network, and maybe it's affecting a bunch of your sites. So what might be useful there is if you can send me some information about what you're seeing there.  

#### [0:41:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2460) |  You can send it to me by

email or on Twitter or something. Let me just drop my email address here. CHRISTIAN NEUWERTH: OK. JOHN MUELLER: And then I can pass that onto the web spam team to double-check. But in general, these kind of weird link spam networks, they come and go all the time, and they don't have any negative effect on the websites that get linked to. Sometimes it will be that they link to five spammy sites  

#### [0:41:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2490) |  that they want to promote and then

10 good sites that they want to make it look like this kind of spammy site is linking to some good sites as well. And that sometimes includes good sites where people don't know what is happening. But we have a lot of practice with these kind of networks, so we should be able to just catch that so that you don't have to do anything there. And again, I'm happy to double-check with the web spam team on this particular case.  

#### [0:42:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2520) |  CHRISTIAN NEUWERTH: Yeah, we're really-- JOHN MUELLER:

Just to-- yeah. CHRISTIAN NEUWERTH: Yeah, we're really sure this must be spam and must be annoying for other SEOs, too, so it would also be great if we could just remove them, maybe, from the disavow file, because it's already hundreds of domains which are-- yeah, make it a little bit more hard to check the disavow file name. JOHN MUELLER: Yeah. OK. CHRISTIAN NEUWERTH: Thank you very much for the email, and we'll send it to you. JOHN MUELLER: All right. Thank you. Cool.  

#### [0:42:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2550) |  Then there's a question about-- NANDHINI BABU:

[INAUDIBLE]. JOHN MUELLER: --how-- NANDHINI BABU: How do we prevent Google from being crawling or indexing our staging server? JOHN MUELLER: How to prevent Google from crawling the staging server? There are multiple ways. So the-- people do it in multiple ways. I think the important part is that you don't link to it, because if we don't find it, then we can't crawl it.  

#### [0:43:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2580) |  But sometimes it still happens. Ideally, what

you would want to do is provide some kind of server side authentication on the server so that normal users, when they go there, they would get blocked from being able to see the content. So that would include Googlebot. And you can do that, for example, on an IP address basis. You can do it with a cookie. You can do it with normal authentication on the server. Anything where you have to kind of prove  

#### [0:43:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2610) |  that you're the right person so that

you can actually look at that content. I think that's generally the best approach for staging servers, because it means-- NANDHINI BABU: Can you use robots.txt to prevent it? JOHN MUELLER: You can also do that. That's the next part I was getting to. I think using authentication is the cleanest, because it's something that means you don't have to change the normal settings on the site itself.  

#### [0:44:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2640) |  So, in particular, robots.txt, but also no

index meta tags, for example, on these pages. Because it's a very, very common thing that you set up your staging site with a new design and you have a robots.txt saying, don't crawl any of this, Google, because I'm still testing things out. And then you push that staging site to production to make it live, and you accidentally include that robots.txt file. Or you accidentally include all of these no index meta  

#### [0:44:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2670) |  tags and then suddenly, your normal website

is blocked from crawling, or your normal website is blocked from indexing. NANDHINI BABU: [INAUDIBLE] of examples of that. JOHN MUELLER: Yeah, I think it happens to pretty much, everyone. So it's-- yeah, it just happens. But ideally, I would just use authentication somehow with staging sites so that you don't have to kind of think about the robots.txt and the meta tags. If you can't do authentication, then robots.txt  

#### [0:45:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2700) |  is a good way to help with

that as well. What will happen with robots.txt is if people are explicitly linking to your staging site, then we might index the URL of the staging site. So if you do a site query for staging.yourdomain.com, then maybe you'll see a bunch of URLs that are shown there and they all have this snippet saying we can't tell you what this page is about because we can't crawl it.  

#### [0:45:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2730) |  So that's something that is a little

bit confusing in the beginning, but it's not problematic, because your normal website will rank for your normal queries and the staging website-- we don't know what is there, so we don't show it in the search results. We can show it if someone explicitly knows that the staging site is there. But we wouldn't show it in the normal response. So I'd say ideally, authentication, and then robots.txt if you can't do it any other way.  

#### [0:46:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2760) |  And no index-- I would try to

avoid doing that on the staging site, because it's really easy to accidentally push a set of changes with the no index meta tag on your pages. NANDHINI BABU: OK, thanks. JOHN MUELLER: Sure. If one of our keywords is at the zero position and also shown in the fourth position, how will it be shown in Search Console? So in Search Console, we start at 1 when it comes to ranking.  

#### [0:46:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2790) |  So if your page is being shown

as a featured snippet, which is what sometimes people call the zero position in the search results, then we would call that position 1. And if it's additionally shown in other places in the search results page, then what happens with search console is we decided to count the average topmost position.  

#### [0:47:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2820) |  So that means for a search results

page, where you're ranking number 1 and number 4, we would count that as ranking number 1. In particular, for your website, that kind of makes sense, because you're visible and in the first position. Similarly, if you're visible with multiple URLs from the same website in the same search results page, so perhaps your-- I don't know, your product page is ranking number 5 and then your home page is ranking number 6,  

#### [0:47:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2850) |  then what would happen there is on

a site level, when you look at the queries, we would count that as ranking number 5. So that's the topmost position there. If you look at it on a per URL basis, then we will show your ranking once-- one URL at number 5 and one URL at number 6. So it depends a little bit on how you look at it in Search Console, if you look at it on a query level or on a URL level.  

#### [0:48:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2880) |  But in general, when we count the

rankings, we count the topmost position in that search results page, and we average that across all of the search results that are shown. Also, it's worth keeping in mind this is not a theoretical ranking, but based on what was actually shown to users. So what can sometimes happen is that your page is shown in the featured snippet for a short while, and then afterwards it's shown in the normal position  

#### [0:48:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2910) |  for a longer period of time. And

then you have kind of that number 1 position and then the number 4 position. And if you look at it yourself manually afterwards, it might be that you look at it and you see, oh, I'm only ranking at number 4. Why did Google ever say I was number 1? And that ranking that we show there, the position, is really based on what we showed people at that time. And sometimes it can happen that very few people see it at position 1, but because that's  

#### [0:49:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2940) |  where we showed it, that's how we

would count it. Let's see. Another one that is kind of an interesting question, where probably we will go on forever. I have a bit more time, so if you all want to stick around a little bit longer, that's fine, too. In your webmaster guidelines, the first bullet of things to avoid is automatically generated content, and yet lots of sites do that.  

#### [0:49:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=2970) |  So it kind of goes on with

an example. And you can find examples like this in Finance. You can even find it in Google News. What's up with this guideline in the real world? So I didn't double-check that specific example that you have there. But I think this is a really interesting question. And I think it's something that will probably evolve over time.  

#### [0:50:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3000) |  In particular, there are two aspects that

I see that are kind of common. I don't know-- let's see. Maybe three. Well, let's talk about three things. So on the one hand, this guideline was primarily put up because a lot of spam is completely auto-generated, and we need some kind of a way to take action on that, and this is kind of the guideline that we chose to do that with that. So a lot of spammy sites will take a list of keywords  

#### [0:50:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3030) |  and they will automatically generate fake sentences

that make absolutely no sense, where when a user looks at those pages, they're like, there is nothing useful here, but there are ads, so they click on the ads. And the webmaster guidelines here give us a little bit more room to say, well, this is a completely auto-generated site that makes no sense to keep. Therefore, we will remove it from Search.  

#### [0:51:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3060) |  Another variation of this that we sometimes

see from spammers is that they take existing content and they run it through an automated translating machine, and they use the automatically generated translation and try to rank with that. Where essentially, the translation again is something that when a user looks at it, they would say, well, this doesn't make any sense. It's like, it's in the right language, but the content is not comprehensible. So that's, again, something that we would treat as automatically generated content and say  

#### [0:51:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3090) |  we need to take action on this.

The examples that you're kind of pointing at are things where I think it's almost going in the direction of, well, you're just providing a table of data in a little bit more understandable way. So you could imagine things like a weather report in a sidebar, where you could just show, like, the weather is this many degrees and it's cloudy kind of in a table.  

#### [0:52:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3120) |  Or you could take that information and

just make a sentence out of it, which might be easier for people to read. And this kind of reformatting some amount of data that you essentially have and providing that in a way that is more readable, I don't see that much of a problem with that. I prefer to make sure that the rest of the page has sufficient value so that when people go to that page,  

#### [0:52:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3150) |  they actually find value there, that they

actually think this is something that is useful to them. After they search in the search results, they clicked on something and they found something that was useful. And that's the kind of thing where I kind of say, well, if it's useful to users, it doesn't really matter completely where it actually came from. So that's, I think, one aspect where things are a little bit different compared to a spammy site that just auto-generates  

#### [0:53:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3180) |  gibberish content. The other more almost, I

guess, future-looking aspect is the various machine learning algorithms that try to generate text as well based on some amount of starter information. And more and more, the examples I'm seeing there are that these algorithms are able to generate something that is actually pretty understandable and actually pretty useful where, if you feed it  

#### [0:53:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3210) |  the right amount of information in the

beginning, it will be able to take that content and write it up in a way that is actually really easy to understand and provides a lot of value. I don't know if that's that far now that I would say that this kind of auto-generated content is completely fine and nobody will kind of notice that it was generated by a script rather than a human, but maybe a few years down the time, down the road,  

#### [0:54:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3240) |  I could imagine that that happens. And

at some point, it'll be that when we manually look at these pages, you can't tell if it was written by a human or written by some kind of advanced machine learning setup. And at that point, does it really matter if it was generated automatically or not? From a user's point of view, it has the same value. So with that in mind, I think at some point in the future, we will have to revisit this guideline  

#### [0:54:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3270) |  and find a way to make it

a little bit more granular in that it kind of differentiates between these totally spammy uses of auto-generated content and the actually pretty useful uses of automatically generated content. So one example that I've seen before is with regards to earthquakes, for example. That-- I don't know if this is actually still running, but I believe at some point, this  

#### [0:55:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3300) |  was set up by some news site

or some government agency, where if their sensors detected that a big earthquake took place in a certain location, they would automatically generate a page for that with some automatically generated content around that, which would be something that we could find in Search. So before any human reporter is able to actually sit down and write something up and say, well, this happened in this location and it was this strong and likely had this effect,  

#### [0:55:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3330) |  and if you're in this region you

should watch out for this and that, that's something that could be generated automatically, and that does have a lot of value to users as well. So that kind of differentiation between the different types of auto generated content, I imagine that's something that we'll see more and more coming up as a topic over the years. So I think, especially if you're looking at cases where, like, a site has 9,000 pages that are generated  

#### [0:56:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3360) |  with a template and you look at

those pages individually, you think, well, OK, they were generated by a template, but they're still actually pretty useful. Then that's the kind of thing where I could imagine that, though, the web spam team at some point would say, well, do we need to take action on this just because something happened to make these pages available? Or are these pages-- or do we need to take action on these pages based on the content that they're actually providing  

#### [0:56:30](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3390) |  and the value that they are providing

to users or that they're not providing to users? So that's, I think, will be an interesting thing to follow. OK. I think we're slightly at time. So what I'll do now is just stop the recording. You're welcome to stick around a little bit longer if you'd like to continue chatting. But I'll stop the recording so that it's  

#### [0:57:00](https://www.youtube.com/watch?v=kPVNl7btFHQ&t=3420) |  kind of a reasonable length. Thank you

all for joining, and I wish you all a great weekend. And if you want to stick around, feel free to stick around, and otherwise, feel free to jump on in in one of the future Hangouts. Bye. MOSHE MA-YAFIT: Thanks, John. Have a nice weekend. JOHN MUELLER: Thanks.  