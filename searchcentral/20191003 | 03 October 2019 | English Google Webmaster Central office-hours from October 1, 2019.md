[![English Google Webmaster Central office-hours from October 1, 2019](https://i.ytimg.com/vi/iK8tGopTa-A/hqdefault.jpg)](https://www.youtube.com/watch?v=iK8tGopTa-A)

## English Google Webmaster Central office-hours from October 1, 2019

This is a recording of the Google Webmaster Central office-hours hangout from October 1, 2019. These sessions are open to anything webmaster related like crawling, indexing, mobile sites, internationalization, duplicate content, Sitemaps, Search Console, pagination, duplicate content, multi-lingual/multi-regional sites, etc. 



Watch out for new sessions, and add your questions at https://www.youtube.com/user/GoogleWebmasterHelp/community



Feel free to join us - we welcome webmasters of all levels!



#### [0:00:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=0) |  JOHN MUELLER: Hi, everyone, and welcome to

today's webmaster central office hours hang out. My name is John Mueller. I am a Webmaster Trends Analyst here at Google in Switzerland. Or at home at the moment. And part of what we do are these office hour hangouts where webmasters and publishers can jump in and ask us any questions around their website and web search. There are a handful of questions submitted on YouTube. We can run through those.  

#### [0:00:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=30) |  But if any of you want to

get started with a question of your own, feel free to jump on in. Or if not, that's fine too. Let me just see. OK.  

#### [0:01:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=60) |  So let's refresh the list and see

what we have for today. From an SEO point of view, is there any difference between posting all external links and citations in footnotes? Like, for example in scientific papers instead of putting them in text. Is there any guideline suggested? So, practically, there is a difference there. In the sense that when we find links  

#### [0:01:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=90) |  within the content of a page, or

within like, extra context, it's a lot easier for us to understand what this link is about. So you could imagine, like, if we have one paragraph of text and there is a sentence in there with a couple words that are linked directly, then there's a lot of context for that link that tells us a little bit more about what is being linked to. And that helps us to better understand the page that's being linked from there.  

#### [0:02:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=120) |  In comparison, if you had all of

the links in the footnotes, essentially, a block of links that are all together and no additional text around that block, as a human reader, you might be able to look at that and say, oh, there is a small number. And that matches a small number somewhere in the text. But, essentially, it's completely separate. It's a block of links on its own. It doesn't have a clear anchor text. Then that's something that, from our point of view, would make it a lot harder to understand  

#### [0:02:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=150) |  what those links are about. So, especially

if you're doing things, like internal linking within your website, you want to make sure that you give as much context as possible for the pages that you're linking to. So from that point of view, I would strongly recommend, like, just putting those links normally within the context in the place where users would find them as well and where users will be able to use them directly. So instead of users having to kind of like, scroll to the bottom and try to search for link and click on it,  

#### [0:03:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=180) |  they can access it directly. Some sites,

like, I guess Wikipedia is probably one of the more prominent ones, tend to do it with footnotes. And that's just the way that they do it. From my point of view, if you're making a normal website, then I would try to stick to kind of the trusted model of having links with clear anchor text on a page and those links being placed normally within the page  

#### [0:03:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=210) |  itself. Does the meta robots max snippet

also apply to the featured snippet? Does the data no snippet attribute not count towards the ranking of the page? Or what's up with those kind of meta text? So the max snippet meta tag-- robots meta tag-- essentially tells us how long in characters  

#### [0:04:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=240) |  a normal text snippet can be. And

that applies regardless of where it's shown. So that's something if that result is shown in the top in a featured snippet, or somewhere in between of the featured snippet, or as a normal blue kind of blue link result, then the max snippet link is essentially the number of characters that we would show for that normal result. There's one exception offhand that I can think of.  

#### [0:04:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=270) |  I'm sure there's something else I'm missing.

But if you're using structured data to trigger a specific rich result type, then that would not apply. For example, if you have structured data to create a recipe, and that recipe is shown as a recipe result in the search results, then it doesn't really make sense to count the characters there. Because these are different elements that all kind of build up this search result together. It's not one block of text that you can clearly kind of  

#### [0:05:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=300) |  limit by number of characters. So if

you have structured data and your page is being shown with that appropriate rich result type, then the max snippet length would not apply there. But for the normal search results, it would definitely apply. One thing also there to keep in mind is that with some types of search results, we need to have a snippet that we can show. For example, if you have a featured snippet  

#### [0:05:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=330) |  and you set the max snippet length--

I don't know-- to one character or something else, then it's probably likely that we wouldn't show that as a featured snippet. Because with one character, there's not a lot of information that we can give. So from that point of view, if there's a specific search for the type that you're aiming for, and you want to use these no meta tags, then make sure that you're not kind of blocking that search  

#### [0:06:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=360) |  result type from appearing. Similar with image

results, if you say, I don't want to have any image preview shown, then we won't be able to show any image previews. And any search result that depends on an image preview to function, that wouldn't be usable from that point of view. With regards to the data no snippet attribute, I think that's a pretty cool attribute that lets you specify within a page which parts of the content  

#### [0:06:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=390) |  you don't want to have shown in

a snippet. And that also applies to all places where we would show the normal text snippet. It wouldn't apply to structured data. So again, if you're doing a recipe, and you mark up a recipe, then we assume the kind of the structured data that you specify for a recipe is something that can be used as a recipe. Otherwise, if you don't want to view as a recipe, then don't use the mark up. But, essentially, for any normal text preview  

#### [0:07:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=420) |  that we would show, that's something where

that would apply. And it doesn't change ranking. It really only changes what we would show in the preview. So if there is something important that's in there, we would still rank that page for that content. We just wouldn't show that in the preview. Let's see. It seems that the title tags have again been reduced.  

#### [0:07:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=450) |  And now my title description appears truncated.

This is fine since I could remove your words. And essentially, I think it goes to, well, like, why do you keep changing the title tag length? I keep working on my titles, and then you change the length again. I realize that can be frustrating, especially if you're fine tuning your titles like that. However, I don't know if it's always worthwhile to actually  

#### [0:08:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=480) |  fine tune the titles like that. Because

it's mostly because of two things. On the one hand, the title tag-- the title length that we display can change over time, like you saw there. And, on the other hand, sometimes we automatically generate titles for your pages, depending on the query and page to make it easier understandable for users why this page is relevant for their interests. So that's something that can change algorithmically anyway.  

#### [0:08:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=510) |  So you're not always guaranteed that you

will see exactly the title as you have specified on a page. So from that point of view, I'd be kind of cautious about always jumping in and saying, oh. The title is a little bit shorter in the search results. Therefore, I need to rewrite everything. I would kind of let it settle down. And if you see pages where you see the titles are significantly off from what you'd like to have shown, then that's something  

#### [0:09:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=540) |  where I would focus on. And again,

keep in mind we optimize the title based on the query. So, just doing a site query and seeing what titles are shown is not necessarily representative of what a user would see. So, I would double check the queries that lead to your site. You can get those in Search Console. You can look at the impressions. And, based on those queries, try them out. Then, see if the titles that's shown for your pages  

#### [0:09:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=570) |  for those queries is something that you

think makes sense or not. And then based on that, kind of make a list of the things that you'd like to have changed. And work on those titles and try to make them a little bit better. And the same applies to descriptions as well. Descriptions are also generated automatically based on the page and based on the query that the user is doing. So doing a site query to see the descriptions that we will show, kind of the preview text, doesn't necessarily represent what a normal user would  

#### [0:10:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=600) |  see when they search for something very

specific for your website. So, that's two things to kind of watch out for. The other thing, just in general, is that I think we'll continue to make changes in the search results as we see that it makes sense. We test these changes quite extensively to make sure that the things that we change makes sense for the users.  

#### [0:10:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=630) |  It's not that someone just goes in

there and says, oh, let's see what happens when I had five more characters or when I change the font size by two or three pixels. It's something that is really tested intensively. It's something where we test it together with the quality raters, kind of on a manual level to see if it still works out. And we test it algorithmically as well in that we do kind of A/B testing with the search results all the time. But we have, I don't know, hundreds of tests  

#### [0:11:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=660) |  running at the same time. And most

of those tests, we notice, well, this is not really working out the way that we expected or that we hoped it would happen. So we tweak the test. We try it again. And we try it again until we come to a situation where we see, well, actually at the moment, based on what we see the users kind of responding with, this algorithm, this UI change, makes sense. And maybe we should roll it out a little bit broader. So these are things that are usually  

#### [0:11:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=690) |  in the pipeline for quite some time.

And the teams here are constantly tweaking and trying to make things better. I think that's something that every website should do. If you're active online, the internet is constantly evolving. The user needs are constantly changing. The devices they're using kind of change over time as well. So you really need to constantly kind of be on top of things, in A/B tests, and figure out where you can improve things.  

![](https://i.ytimg.com/vi/iK8tGopTa-A/hq1.jpg)



#### [0:12:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=720) |  Sometimes incrementally, sometimes you can improve things

quite extensively as well. Let's see. I seem to have missed that last pop up. OK. We bought a domain from our competitor and redirected to ours. Is there any way to find the old disavow file or to disallow new backlinks to this domain? Even though it's in Search Console,  

#### [0:12:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=750) |  it's not showing up in the disavow

tool. So, they're kind of two things here. On the one hand, I believe the disavow file will be visible when you have verified normally. However, to verify it so that you can use it in the old Tools from Search Console, you need to make sure that you verify it as kind of an old style property. So with either the meta tag or the HTML file,  

#### [0:13:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=780) |  or one of the other methods there.

And, based on that, you should be able to download a disavow file once you have that verified. I don't know if, like, if you would see a lot of value out of redirecting a domain from someone else's website to your website. But, if you bought that domain, and especially if you're in a situation where you're saying, well, there is a lot of direct traffic to this old domain.  

#### [0:13:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=810) |  And it's something where I think my

site would be a good match, then that might be a chance to kind of look into that. But again, you'll find the disavow file in the disavow tool. And for that, you need to create an old style property. And you need to create the old style property that was used to upload the disavow file. So things like HTTPS or HTTP dub dub dub or non dub dub  

#### [0:14:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=840) |  dub, those are things that you need

to keep in mind. You can just add all four versions and see where the file is. That might be an easier approach than trying to figure it out. Last month, I launched a new website and have been releasing content weekly on the blog, over thousand words weekly, which is getting good views due to social sharing. After one month, I'm not ranking in the top 100  

#### [0:14:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=870) |  for any of my keywords. And they

tried Search Console and they used the feedback link on the bottom of the search results pages to let Google know about this. But is there anything that can be done there? So, I think, in general, you can't always expect to show up in the search results, especially with a new website, just because you're creating  

#### [0:15:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=900) |  content that's out there. So that's something

where it can take a bit of time for things to settle down for our systems to understand. But actually this is a pretty good website. And it can take a really long time, especially if it's on a topic where there's a lot of competition. So just because you have a new website and you're regularly creating content for it, doesn't necessarily guarantee that you  

#### [0:15:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=930) |  will be shown for all search results

so you're creating content for. The other thing to keep in mind is, since you mentioned the over 1,000 words, we don't have any limit or any kind of guideline with regards to how many words per page or how many words per month you should be generating. Essentially, we try to make sure that we show relevant search results to our users. And sometimes, those pages that we link to have a lot of text. Sometimes those pages don't have a lot of text.  

#### [0:16:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=960) |  There's no magic number from our side

where we would say, this is the number of words that you need to target, and then we will show you in the search results. So that's another thing to keep in mind there. I would definitely check out the SEO Starter Guide. We have that link in the Help Center. That has a lot of good information there. I think, especially if you're targeting kind of this commercial niche of insurance  

#### [0:16:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=990) |  where it's probably pretty competitive, then I

would make sure to try to get some help from someone who's a little bit more experienced with regards to making websites kind of do well in search. Sometimes you just need a little bit of tips. Sometimes you really need more long term help if it's something where you really need to support a website for the long run and make sure that it's kind of competitive enough  

#### [0:17:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1020) |  to match what others are doing in

that space. Is there any way to tell Google please crawl me more and ignore the duration time as the page gets slower? No. Not necessarily. So what we have in Search Console is the crawl rate setting, which you can use to give us a little bit of information. But it's not something that would override  

#### [0:17:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1050) |  our general systems where we would say,

try to protect your server from being overrun. So that's something where, if our systems feel that your server is not able to cope with our crawling, then we will slow down. Because we really don't want to cause any problems on a website. We really want to make sure that we can crawl your pages, we can index them kind of reasonably. And that the majority of your traffic is actually for your users. It's not that you're making a website  

#### [0:18:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1080) |  and creating the content, providing it just

for Google. You're kind of creating it because you have users out there who you hope would be attracted to your content. So we want to make sure that your website stays available for them rather than kind of running it to the ground to crawl every last bit. Let's see-- JARDA HLAVINKA: Hi John. Can I ask you a [INAUDIBLE] question please? JOHN MUELLER: Sure. JARDA HLAVINKA: Yeah. [INAUDIBLE] I definitely know about the  

#### [0:18:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1110) |  crawl rate settings in Google Search Console.

And I talked at the Wednesday with Martin Split about this. But we didn't find it any good solution. But the thing is that the page is very large. And you can click Read More in my comment.  

#### [0:19:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1140) |  There is a lot of information. It

is not only about crawl rate settings in Google Search Console. JOHN MUELLER: OK. Yeah, OK. Go ahead. Sorry. JARDA HLAVINKA: No, no. Do want to tell something? Or-- JOHN MUELLER: Oh no. JARDA HLAVINKA: OK. OK. OK. Thank you. Thank you. The thing is that the website is so Angular 1. And we use Chrome headlights for dynamic rendering.  

#### [0:19:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1170) |  And we have about 500 millions pages,

URLs. And we only cache about 6 millions. And the others are rendered on the fly, like on the Chrome headlights. And it takes about 1.5 seconds per URL. So it is very slow. And the thing is, that when you crawl, the set of 6 millions URLs, which are cached  

#### [0:20:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1200) |  and which are most important for us,

is OK. Because of they are pretty quick. But in case Google is-- I don't know how to express it. When you have enough of these URLs, and try to find another ones, try to crawl another ones which are not in the cache, it takes about 1.5 seconds per URL. And it means that the mean of duration time or render time  

#### [0:20:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1230) |  or crawl time is something like, it

goes really higher. So, so I know that in this case, it looks like Google will says to himself something like, yeah, maybe I'm the bad guy who is slowing the web server. So maybe I have to slow. But I don't want him to. I want him to grow as much as possible, because we have a lot of servers. But our problem is that the meantime  

#### [0:21:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1260) |  for rendering, it's about 1.5 seconds per

the URL, which we can get better would this technology like Angular 1 and Chrome Headlights. We know that we have to change the technology to, I don't know, React or something like this, maybe PHP or something. Doesn't matter. But the thing is that in this technology set up,  

#### [0:21:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1290) |  I want to tell the world, crawl

me as fast as you can. JOHN MUELLER: Yeah. I don't think that that would be possible. Yeah. So, let's see. I think-- so in general, that's something where we would probably slow down. I would also not use the crawl rate setting in a case like that.  

#### [0:22:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1320) |  Especially for really large sites, if you

use the crawl rate setting, the setting maximum value is probably too small. JARDA HLAVINKA: Yeah it is. JOHN MUELLER: For your needs. So instead of setting it to the maximum value, I would disable that setting. JARDA HLAVINKA: It is. JOHN MUELLER: So that we can crawl as much as possible. I think you linked to the domain in the question. I didn't see that. It looked like the question was complete there. And looking at that, it is the case  

#### [0:22:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1350) |  that we're kind of limited from crawling

because we're kind of worried for probably speed reasons at the moment with regards to what we can crawl. I don't know when you switched over to the new setup. It looks like mid-September, we were crawling a little bit better. But, it fluctuates a little bit. So it's really hard to say much there.  

#### [0:23:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1380) |  But it's really something where we see

the content. We recognize that sometimes it gets slow. So we limit the crawling that we do there to make sure that we're not causing any more trouble with regards to that. So, one thing you could potentially think about is moving the kind of pre rendered content to a different host.  

#### [0:23:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1410) |  So that we can separate these two

configurations. I don't know if that would make sense in your case, or if content would move between the kind of pre rendered state and the on the fly rendering state. That might be something that would work. I don't know if that makes it any easier on your site compared to increasing the number of pre rendered pieces of content. What I would also kind of double check  

![](https://i.ytimg.com/vi/iK8tGopTa-A/hq2.jpg)



#### [0:24:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1440) |  is the crawling that we do versus

the pre rendered content that you provide, if that kind of is in line. Not that we're spending all the time crawling pages from your point of view you think are irrelevant. That might be something to kind of look at. You could see that in your server logs where you pull out the URLs that we crawl and compare that to your list of URLs that you would have pre rendered.  

#### [0:24:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1470) |  But yeah. I don't see any other

big approach there. Because if we see that things are slow, then we try to adapt and say, well-- usually, the speed thing is not something that we would kind of like say, well, your server appears to be really slow. But it's something where we would say, we want to limit the number of simultaneous connections that we have to your server.  

#### [0:25:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1500) |  And if it takes, I don't know,

two seconds to load a page from your server, and we want to limit the number of connections that we have simultaneously to, I don't know, 10 or some other number, then, obviously, there's only a maximum number of pages that we can crawl per second. So that's kind of where that speed aspect flows in. So, if you need to serve these pages slow,  

#### [0:25:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1530) |  then I don't know if that's really

kind of the easiest approach there. Maybe another thing that you could do, depending on the pages and how you have them set up, is to have some part of it kind of server side rendered and another part just dynamically included, so that the server side rendering doesn't need to do all of the work and can be done a little bit faster. I don't know how you pages are built up or how your infrastructure is set up.  

#### [0:26:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1560) |  That might be an approach as well.

JARDA HLAVINKA: OK. Thank you for help. Thank you very much. I will take a look deeper. Thank you. JOHN MUELLER: Yeah. Let's see. What else? We have that's anything that's been added. It looks like most people added their questions to the Friday hangout, which is the top line in the YouTube list, which is not really the one for now.  

#### [0:26:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1590) |  But, that's fine. We can look at

that on Friday. I want to move my best articles from the old domain to a brand new domain. Is there any correct way to do it? Will cross domain canonicals help in this situation? Or should I no index the old ones? I don't want to 301 them as I don't want to send any links or content quality relevant signals to the new domain. I think you probably have to make up your mind there with regards to old domain and new domain.  

#### [0:27:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1620) |  So you can't kind of forward signals

to the new domain for these pages and say you don't want to forward signals. Because either you're forwarding the signals or you're not forwarding them. It doesn't really matter if you're using redirects, if you're using a rel canonical, if you're using JavaScript redirect or meta refresh, or anything like that. If you're telling us that one page has moved from one URL to another URL, then we will forward those signals  

#### [0:27:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1650) |  to that new URL. So, in that

case, if you want to move some of this content from one URL to the other, then using redirect is probably the best approach. If for practical reasons, you need to keep it on two separate domains, then using a rel canonical is a good approach. But in both of these cases, we forward the signals from one URL to the other URL. So, that's something to just kind of to worth keeping in mind in that you can't have both  

#### [0:28:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1680) |  your old URL ranking with like full

power and your new URL ranking with full power if you're telling us that the old URL has moved to the new one. So pick one, whichever one you want. Personally, I usually recommend trying to keep things on as few domains as possible. On the one hand, for practical reasons, because you need to maintain all of this somehow. On the other hand, it makes it much easier for a good domain  

#### [0:28:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1710) |  to be really strong if you have

all of the content that you care about on one domain. But, ultimately, that's up to you. Sometimes they're also kind of marketing things involved here as well where maybe you're using your URL in offline advertisements, then that's something where you might want to have that old content available on that URL as well as having a rel canonical set up to the one that you prefer to have indexed.  

#### [0:29:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1740) |  Let's see. Hi. KRISTINA AZARENKO: Hi. So,

I have a question about translated content. For me, it makes sense that if you translate not automatically, but if a translator translates content, it should be unique. But I see so many questions and people are kind of confused that they think that it can also be treated as [INAUDIBLE] content. So yeah, I wanted to run it by you.  

#### [0:29:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1770) |  Is it unique content if you translate

it like a real person translates it? JOHN MUELLER: Yes. Absolutely. I think the easiest way to look at it is, these are completely different words. So if you translate the content from one language to another, it's completely new piece of content. It's not something where we would say, this is a variation of the previous one. It's essentially completely new piece of content. You can link the two with hreflang if you wanted to do that.  

#### [0:30:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1800) |  But you don't need to. Sometimes people

just have a translator version of the content and have that completely separate. But it's definitely not seen as duplicate content. I think the one case where it kind of gets into the duplicate content discussion is when you have content that you're translating from American to British, for example. And you're just tweaking the original letters to match what the other site is saying. Then, that's something where we might say,  

#### [0:30:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1830) |  well, this chunk of text is essentially

the same as that chunk of text. But if these are different languages, it's completely different content. KRISTINA AZARENKO: Yeah. OK. Thank you. And one more question about structured data. So, if you have, for example, one kind of structured data which is invalid or has errors or spanning some kind. So for example, you have product structured data and structured data for organization and [INAUDIBLE].. But only one of these structured data sets is invalid.  

#### [0:31:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1860) |  Does this mean that Google will just

disregard all of them? Or only this type which is invalid or spamming? JOHN MUELLER: So, it depends on how we would recognize that and ignore it. If it's something that the web spam team manually recognizes, then that's something where we've shifted to a more granular model where we can try to isolate that specific type  

#### [0:31:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1890) |  of structured data. So if, like, your

organization markup is bad, but your product markup is good, then you could get a manual action for just that organization markup. And then just the organization markup would not be shown in search. It's similar if the markup is essentially just broken, where we can't process it properly. If the organization markup is broken, the product markup is OK, then we would just  

#### [0:32:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1920) |  have the product markup. And we can

show the product markup. So it's not the case that if one is bad, that we would ignore it completely. It's something where we try to do it on a piece by piece basis. I think it would get tricky if you had multiple products across your website, each with the product markup. And some of those product pages have spammy product markup and some of them don't. Then the web spam team might still say, well, for this domain, or this website overall,  

#### [0:32:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1950) |  the product markup contains spammy elements. So

we won't show any of the product markup. And then that would apply across all of these pages. But if these are different types of markup, then we would try to treat them separately. KRISTINA AZARENKO: OK. Yeah. Thank you John. JOHN MUELLER: Sure. All right. I imagine all of the questions are lined up for Friday. So it'll be really busy then. But like, more time for you all to ask questions  

#### [0:33:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=1980) |  live, in that case. If there's anything.

Oh my gosh. We ran out of questions. Let's see. Yeah I don't know. KRISTINA AZARENKO: OK, one question for me, then. JOHN MUELLER: Go for it. KRISTINA AZARENKO: So, the domains which have not the comma at the end, but dot io,  

#### [0:33:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2010) |  it's initially geographical identification. But I see

that many tech companies, startups, they tend to use io at the end. Does this mean that Google just treats these more like a general country level domain? Or real country level domains, if it make sense? JOHN MUELLER: Yeah. We have a list of country code top level domains that we treat as generic top level domains.  

#### [0:34:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2040) |  I think that's in the Help Center

somewhere. I don't know offhand if io is on there. I imagine it is. But things like CO, instead, of Colombia-- lots of people use it for company. That's something that's on this list. A really simple way to double check if you have the website already is to use Search Console, verify that domain, and go to the international targeting tool and see if you can set an international target.  

#### [0:34:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2070) |  If you can set the kind of

country target, then we treat it as a generic domain. You can target any country that you want. Or you can just say, I don't want to target any country. I prefer to keep it generic. KRISTINA AZARENKO: OK. Yeah. Thank you. JOHN MUELLER: Sure. The same applies also for all of the new domains and for all of the new domains, we treat them as generic domains, even if they look like country or location domains.  

#### [0:35:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2100) |  So for example, if you have dot

Berlin, or, I don't know, dot NYC is one, I think. Then, they're essentially sold as a location specific top level domain. But we treat them as a generic top level domain. Because we really haven't had any experience that this is limited just to one target audience. So if you have something like dot NYC and you want to make sure that you're targeting users  

#### [0:35:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2130) |  in the US, then make sure that

you check out the international targeting setting on Search Console so that you can kind of really geographically target the users that you're looking for. KRISTINA AZARENKO: OK. Yeah, thanks. JOHN MUELLER: Cool. Wow. I did not expect that we would run out of questions. Let's see. One in the chat. If we block Google Bot, but do not mark our pages as no index,  

#### [0:36:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2160) |  will our pages still be indexed in

Google? In our case, we have a staging site that we would like to use some tools to crawl for errors. But they require the page to be marked indexable. So it depends on how you block Google Bot. Our recommendation for staging sites is to try to block Google Bot on a server level. So either with HTTP authentication or with IP address whitelisting that you kind of let  

#### [0:36:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2190) |  the users or the tools that you

need to have access to your site have access, and block everyone else. If you do it like that, then make sure that we can't crawl anything from your website. On the other hand, if you use a no index, then we might still call some of those pages from your staging site. Or if you use a robots text file, then it can happen that we index pages without crawling them. So we'll just index the staging site URL.  

#### [0:37:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2220) |  And if you do a site query

for the staging site, maybe we'll have some of URLs from your staging site that we know about, but haven't been able to crawl. So both robots texts and no index are two things that I would not recommend for saving sites. Instead, try to use either authentication. So username password setup. Or with work with IP addresses that you're explicitly allowing the IP addresses or the tools and the users  

#### [0:37:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2250) |  that you'd like to have access for

these pages. So, in both of those cases, we wouldn't be able to crawl as well. And that would let us kind of understand that this is not something that you want to have indexed. The other thing with robots.txt and no index is that, it happens extremely frequently, even for really large websites, that they push a version of a new website live that has everything blocked by robots.txt or everything with a no index meta tag because they  

#### [0:38:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2280) |  forget to double check that. And if

you're using authentication or IP addresses, then it's really trivial to recognize that you left the wrong settings in place and you need to fix that before kind of letting it run loose. GEORG PAGENSTEDT: I would have another question. JOHN MUELLER: Sure. GEORG PAGENSTEDT: I'm not sure if you will answer. But is a Google update, the core algorithm update already over? Or is it still running?  

#### [0:38:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2310) |  JOHN MUELLER: Which update do you mean?

GEORG PAGENSTEDT: The September core-- I don't know how you call it. Algorithm update, I think it is. JOHN MUELLER: I don't know. To be honest. Probably, it's completely rolled out. Usually, updates like these take a couple of days, maybe a week to roll out completely. But it's something where we tend to announce them  

#### [0:39:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2340) |  when they happen so that people know

what's roughly happening. But, it's really hard to give an exact starting and end time. Because there are always so many other things that are happening in Search. GEORG PAGENSTEDT: OK thank you. JOHN MUELLER: You're welcome. Another question from the chat. Our WebDev recently locked down access to unknown crawlers in Cloudflare, I guess.  

#### [0:39:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2370) |  I've heard that Google will crawl sites

using non-standard user agents to check for cloaking. Is that true? If so, is there any risk to our content getting crawled based on our recent changes? Is there anything I should advise our WebDevs to do to accommodate non-standard instances of Google? So, for web search, when it comes to crawling and indexing content, we use the normal Google Bot. So, if there's something on your site  

#### [0:40:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2400) |  that you want to make available, you'll

see us crawl it using normal Google Bot user agent. The exact user agent can vary a little bit. So especially on mobile, we try to match it to whatever device and settings kind of makes sense for that. We're also working on adjusting the Chrome version in the user agent so that it matches more what we use for rendering. So that's kind of exact level will vary over time, or depending on how we access the page.  

#### [0:40:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2430) |  Also desktop and mobile have slightly different

user agents. But they all have Google Bot specified directly in the user agent. So that's something explicitly to watch out for. And if it doesn't have Google Bot in the user agent, then it's not a crawler from Google Search. So if you're blocking everything that has Google Bot in the user agent, then you would be blocking Google from being able to crawl and index that content for Search. We do have a variety of other user agents that access pages.  

#### [0:41:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2460) |  So, things like for ad sense, we

need to be able to access the page to see what this page is about so that we can show relevant ads. I suspect there are other similar crawlers as well. I think we have a list of those in the Help Center, or a list of the more common ones there at least. There's also for Search Console and for site maps,  

#### [0:41:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2490) |  I think, separate user agents that we

would use to confirm verification. If you need to have the site verified in Search Console, then make sure that you're not blocking the bot that checks that this site is verified. Or if you're using DNS for verification, then that's one thing less to worry about. And I believe for site maps, we also have a separate user agent that fetches a site maps file. Though I'm not sure actually now--  

#### [0:42:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2520) |  probably also contains Google Bot in the

user agent. That's easy to kind of double check as well. But if you're blocking all Google Bot requests to your normal content, and you have a sitemap file, then probably you would be blocking all of the requests for the URLs in the sitemap file that we try to fetch. So that shouldn't make much of a difference there. But again, if you're blocking everything that contains Google Bot in the user agent, then you would be blocking everything that is used for Search.  

#### [0:42:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2550) |  Obviously, that makes our search team kind

of sad if you decide to block all of Google. But ultimately, that's totally up to you. And that's a choice that you can make. Sometimes it makes sense to do that if you have kind of different things that you're testing out and you want to let users take a look, but you want to make sure search engines don't, then that can definitely make sense. Or sometimes you might just say, I want my content online, but I definitely don't want any of it in Google  

#### [0:43:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2580) |  at all, then that's also your choice,

of course. Let's see. I have a question. Past the core update, our impressions on Google Discover were reduced to zero. Does this mean that Discover has blocked us? Does Google Discover have a white labeling process similar to Google News? Google News is still giving us traffic. No. As far as I understand, Discover is a completely organic  

#### [0:43:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2610) |  feature. Obviously, how and why do we

choose to show content in Discover is quite different to normal Search because there is no query involved. And in that sense, sometimes, if you write about content that people really care about, then that's something we could pick up and show in Discover. But it's not guaranteed that we would show it there. On the other hand, Google News, like you mentioned, is built on the setup where sites are kind of double  

#### [0:44:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2640) |  checked and placed into the Google News

corpus based on their submissions. And based on that, we would show them in Google News. So that's completely separate, separate setup from normal Search and from Discover. OK. Yeah. More questions from any of you, feel free to jump in or let me refresh to see what else is on the comment.  

#### [0:44:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2670) |  I think that's just the one that

we just had about locking down access to crawlers. Was that about what you were looking for, Andrew? ANDREW NEVELOS: Yeah. Yeah, we're not blocking Google. But we're blocking a bunch of other automated scrapers from accessing the site. Yeah. JOHN MUELLER: OK. So the other way around? ANDREW NEVELOS: Yes, yeah.  

#### [0:45:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2700) |  JOHN MUELLER: OK. I think if you--

so if you're not blocking Google, one thing I would do is double check the user agent. And then do that reverse IP lookup, which we have specified in the Help Center so that you make sure that you're kind of allowing the real Google Bot, not other tools and scrapers that use a fake Google Bot. Because there are lots of them out there that say, well, I'm Google Bot. You should give me all your content.  

#### [0:45:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2730) |  OK. Cool. I mean, we still have

10 minutes left if people want to ask questions. Feel free to jump on in. One thing I did want to mention as well, especially for this hangout, is we recently announced a Web Master Conference in Mountain View. So if you're in the US, in particular in the region there, then check out the Web Master Central blog,  

#### [0:46:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2760) |  and maybe sign up for that. It's

a free event. We'll have a number of people from the Search site there as well. We'll have some lightning talks, some Q&A with product managers there. I think it'll be pretty interesting. We run these Webmaster Conferences all around the world. And this is, I think, the first one that we're doing in the US. So it's a little bit special for us. But it's not the last one that we're going to be doing, at least according to our plan. So if you can't make it there, then don't panic.  

#### [0:46:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2790) |  I'm sure we'll have more opportunities as

well. But, I'll be going there. So if you want to drop by and say hi, check out the blog and sign up. Cool. Well, if there are no more questions, I guess we can close a little bit early. That works for me too. All right.  

#### [0:47:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2820) |  Oh, here comes one in the chat.

For local businesses building directory listings and citations, do you guys count links from these places like YP.com as backlinks? I don't think we would do anything unique for sites like YP.com. We do take into account links from all kinds of sites. We also ignore links from all kinds of sites when we think that these links are not that relevant for our algorithms.  

#### [0:47:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2850) |  So it's not that we would have

a separate set of, kind of like, links from this site should be treated differently because they belong into this specific category. It's more that, for most sites, it makes sense for us to tweet links like normal links. And if they have a no follow, we treat them as a no follow. If they don't have a no follow, then we can follow that. And for some sites where we're really not sure if there is value in using these links for Search,  

#### [0:48:00](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2880) |  we just ignore a lot of those

links. But, it's not that we would do anything kind of special for directory sites or local business listings. Cool. All right. So, I guess what that, let's take a break here. Thank you all for jumping in and joining us. Hope to see you all in one of the future hangouts as well. And I wish you all a great rest of the week.  

#### [0:48:30](https://www.youtube.com/watch?v=iK8tGopTa-A&t=2910) |  Bye everyone. KRISTINA AZARENKO: Bye!  

