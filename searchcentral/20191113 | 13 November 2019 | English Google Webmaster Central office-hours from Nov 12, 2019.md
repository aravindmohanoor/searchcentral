[![English Google Webmaster Central office-hours from Nov 12, 2019](https://i.ytimg.com/vi/Vej7f43fiyM/maxresdefault.jpg)](https://www.youtube.com/watch?v=Vej7f43fiyM)

## English Google Webmaster Central office-hours from Nov 12, 2019

This is a recording of the Google Webmaster Central office-hours hangout from November 12, 2019. These sessions are open to anything webmaster related like crawling, indexing, mobile sites, internationalization, duplicate content, Sitemaps, Search Console, pagination, duplicate content, multi-lingual/multi-regional sites, etc. 



Watch out for new sessions, and add your questions at https://www.youtube.com/user/GoogleWebmasterHelp/community



Feel free to join us - we welcome webmasters of all levels!



#### [0:00:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=0) |  JOHN MUELLER: All right. Welcome, everyone, to

today's Webmaster Central office-hours hangout. My name is John Mueller. I'm a webmaster trends analyst here at Google in Switzerland. And part of what we do are these office-hour hangouts where webmasters, publishers, SEOs, anyone can join in and ask questions around SEO. A bunch of stuff was submitted already, but if any of you  

#### [0:00:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=30) |  want to get started with the first

question, you're welcome to jump on it now. BARRY SCHWARTZ: I have a question if nobody else does. JOHN MUELLER: Go for it. BARRY SCHWARTZ: I know you hate these questions, and it's been a long time since I asked them. But over the weekend there was probably more chatter in the SEO industry about a Google update that I've seen since maybe, the Penguin or Panda days. I did reach out to certain people at Google  

#### [0:01:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=60) |  to find out-- not you but other

people at Google to find out if there's any statement you want to deliver? Are you aware that I reached out? JOHN MUELLER: Am I aware that you reached out? BARRY SCHWARTZ: Meaning, is there a discussion internally at Google about maybe responding to this question? JOHN MUELLER: I mean, we get questions around potential algorithm updates all the time. So I think that's kind of natural. BARRY SCHWARTZ: Right. JOHN MUELLER: But I-- BARRY SCHWARTZ: You think you can share about this one? JOHN MUELLER: I don't have anything explicit.  

#### [0:01:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=90) |  I mean, on the one hand, we

make changes all the time. So it's kind of the balance between, well, we make all of these changes all the time, and sometimes some individual changes are more visible, or at least more visible by some folks or some sites. So it's kind of the balance between there as, well, do we need to call this one out explicitly, or is this kind of just a normal change? And I don't know what the final decision there is if there's something decided on that yet.  

#### [0:02:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=120) |  BARRY SCHWARTZ: But it's being discussed internally?

JOHN MUELLER: Sure. Sure. BARRY SCHWARTZ: Like it always is, or this one specifically more so than others? Is this anything more than another? JOHN MUELLER: I don't know. Not more than others, I'd say. BARRY SCHWARTZ: All right, I tried. All right, thank you. JOHN MUELLER: I mean, we try to be as transparent as possible about these things. But sometimes there just isn't that much  

#### [0:02:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=150) |  that individual sites can do. So it's

kind of hard to say, well, things change like they always change. BARRY SCHWARTZ: No, it's true. I did ask Danny last week at the conference to release another Google penalty because I haven't seen a penalty announced in a while. So hoping maybe. No, I'm just joking. JOHN MUELLER: No, it's your fault. BARRY SCHWARTZ: Yeah, OK. JOHN MUELLER: At least now people know who to blame. That's good. BARRY SCHWARTZ: Yes. JOHN MUELLER: Cool. All right. Other questions?  

#### [0:03:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=180) |  Oh my god. A really long one

in the chat. In Search Console, can they please use a better word rather than valid when it means something very different, like valid and index? I don't quite understand how you mean that. If you want, feel free to jump on. DAMON HART-DAVIS: Hi, John. Can you hear me? JOHN MUELLER: Yes. DAMON HART-DAVIS: Right. Sorry. I keep nagging you about this in various other places.  

#### [0:03:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=210) |  So if you go into GSE, and

you look in, say, the enhancement reports or something, if I look in my mobile-- and I have a tiny site. I have 230 pages or something each in desktop, AMP, and a light mobile. And it'll go and tell me, you have 123 valid pages under mobile usability. Now, there are no errors in any of the other pages. And what it means by valid there is, apparently, oh, they're valid, and we happened to have indexed them. Now it's inconsistent between different things.  

#### [0:04:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=240) |  If I look at the new thing

that's just come up, the speed one, I get a different number of things marked as valid for mobile under there. So clearly it can't just be valid because otherwise the two of them will give the same number. And all over the place, there are things that say valid which clearly-- there was the old one. The old crawler stats where it says pages, when actually I don't know whether it means pages or objects including other files downloaded. So it'd be really helpful to me, and I've griped about it for a long time.  

#### [0:04:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=270) |  Can we find better words than valid

when we don't mean valid. Because the implication is everything else is invalid, and as far as I can tell that certainly isn't true. JOHN MUELLER: OK. So primarily with the thought that the things that are not valid might be valid? DAMON HART-DAVIS: Yeah. JOHN MUELLER: Except they're just not listed. DAMON HART-DAVIS: Because if they aren't, call me OCD. But I can tell you for sure that I have checked my pages everything going, and I've been doing it since 1995 and I'm pretty sure the pages are valid.  

#### [0:05:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=300) |  I'm quite happy you don't index them

all, but you should say that rather than imply they're invalid, I think. JOHN MUELLER: OK. OK. DAMON HART-DAVIS: That was all really just a gripe. JOHN MUELLER: No. No, that makes sense because I think the these aggregate reports can be a bit tricky in that sense in the way that we report over a sample of the URLs. And we'll tell you, of the sample, this many are valid. But essentially we don't say-- DAMON HART-DAVIS: I have so few I don't think the sampling  

#### [0:05:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=330) |  issue even applies to me. JOHN MUELLER:

OK. Well, it depends a bit on the site. But it's something where, especially in the aggregate reports, we do sample them. So if you add them up, you won't get the same numbers across the different tools because we just look at a sample. And it's basically saying of those that we looked at these are OK. But that doesn't mean the ones that we didn't look at are bad. But I do get that confusion from time to time. So maybe we need to find a different way to frame that.  

#### [0:06:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=360) |  DAMON HART-DAVIS: That's all. I think it's

in different wording. JOHN MUELLER: Different wording is always tricky with translations and all of that. Yeah. OK, cool. DAMON HART-DAVIS: Thanks, John. JOHN MUELLER: Sure. And then a longer question from Matthew. Do you want to maybe just ask that one directly? Do you have a microphone maybe? DAMON HART-DAVIS: It might be helpful to explain to people how it works. I've never used this interface before. I had to guess a few times to press the microphone thing  

#### [0:06:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=390) |  to present. JOHN MUELLER: OK. So there's

a big microphone button if you hover your mouse over your picture, or you can unmute yourself. I can't unmute you. But feel free to jump on in. I'll try to read it out loud and see if I get it right. If not, feel free to correct. We're an e-commerce store. We have many indexed pages, search pages that bring in visitors and who end up purchasing. A few months now, someone has been building thousands  

#### [0:07:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=420) |  of links to our search pages with

illegal and sexual anchor text. Google indexes these dynamic search pages, and we rank very high for terms like "buy Viagra online New Zealand" for several weeks until Google demotes those pages. OK, that seems kind of awkward. If we no index the internal search, we lose converting traffic. On the other hand, manually no indexing these spammy dynamic search pages is too time-consuming because there are  

#### [0:07:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=450) |  hundreds indexed every week. What can be

done? I don't know what the best approach is here. I think in general, with e-commerce sites, what we'd recommend is, if someone is searching for a page, or searching for something that you don't offer within your site, then essentially you're serving an empty search results page which ideally you would be returning a no index for.  

#### [0:08:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=480) |  So anytime someone goes to a category

that doesn't exist or does an on-site search that doesn't exist, it should have a no index on it or a 404 code so that we know this is not something that should be indexed. So ideally, that would be something that your e-commerce site setup does. Theoretically, you could also do this with JavaScript. I don't know. Maybe that's something that could be done with a tag manager as well, where you can dynamically set the no index. I don't know the details of your setup,  

#### [0:08:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=510) |  so it's really hard to say how

exactly you'd be able to do that. The other thing is that, generally speaking, we recommend sites don't let their internal search pages be crawled and indexed because it's very easy to have things blow up in the sense that we have suddenly millions of URLs from your website when actually you just have a few hundred, just because of all these different variations that people can search and that your server returns valid content for.  

#### [0:09:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=540) |  So that's the other aspect to keep

in mind. My general approach to this kind of a problem is to try to find the queries that you do care about, and make sure that those are indexable. And then just make sure everything else is not indexed. One way you could do this is, instead of having search pages that are there to drive traffic, that you have more category pages where you handpick specific categories that you do want to have content shown for,  

#### [0:09:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=570) |  and let those category pages be indexed

while the search pages themselves would be blocked from crawling and indexing. So that would still allow you to have these pages that are focused on a specific category or type of thing that people are searching for without opening things up for anything that anyone is searching for that your site might respond to. So that's the direction I would head there.  

#### [0:10:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=600) |  All right, there are a bunch of

other questions that were submitted on YouTube, but if anyone wants to get another question in on the side before we dive into that you're welcome to jump in. Or if not, feel free to jump in during one of the questions if you have more information or more comments. Oh, we have someone.  

#### [0:10:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=630) |  KOSTTILLSKOTT GUIDEN: Hello? JOHN MUELLER: Hi. KOSTTILLSKOTT

GUIDEN: Hey. I actually have a question for you, if that's all right. JOHN MUELLER: All right. Go for it. KOSTTILLSKOTT GUIDEN: So I'm working on a Swedish website within Swedish markets, so we are writing content in Swedish, et cetera. Some of the subjects, there are not enough information when it comes to YouTube videos.  

#### [0:11:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=660) |  So to give an example, if we

are writing about a vitamin D deficit, for example, and we want to post a video about what we did from YouTube, if we use an English YouTube video, how would that affect us?  

#### [0:11:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=690) |  JOHN MUELLER: I don't think that would

have any negative effect in the sense that we would primarily index your web pages, and we would focus on the textual content on your pages. We would see that there is a video embedded. So theoretically what could happen is that video is associated with your landing page, with the web page that you created. So if someone is searching for that video in video search, we could theoretically show your page  

#### [0:12:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=720) |  as a landing page for that. That

might be kind of awkward, I think, if there's a language mismatch. But for most videos, we also have good content on the YouTube landing pages. So unless this is a video that is really uniquely available on your site, then it's probable that we would just pick the YouTube landing page for searches for the video, and pick your web page for searches around the text that you have on your pages.  

#### [0:12:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=750) |  So it's not that we would say,

this is confusing, and we would treat this as bad. It's more that, well, we don't have this direct connection between the video content and the web page, and that's fine too. KOSTTILLSKOTT GUIDEN: All right. Thank you very much. JOHN MUELLER: Sure. All right, so let's jump into some of the questions that were submitted. Can single page web sites rank if they cover the topic really well, or do Google's algorithms automatically  

#### [0:13:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=780) |  block the page from ranking and prefer

sites with tons of pages? Single page websites can rank. They can rank fairly well. There is nothing against single page web sites, per se. The one thing I would caution, though, is that it's a lot easier to build up your website over time if you keep building on your existing website rather than creating new websites for each individual page that you create. So if this is a topic that you already have a website on I  

#### [0:13:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=810) |  would try to integrate that within your

existing website. If you want to create a single landing page for something like an ad campaign, if that's online or offline, then obviously that's fine. That's something that can work. But in general, I recommend taking a website and building off of that rather than creating individual one-page web sites. Being able to have one stable place for your general web  

#### [0:14:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=840) |  presence makes it a lot easier to

build up value over time because people will know, well, this website has created lots of good stuff. And it's a lot easier to grow that as a website rather than say, well, this one page is actually pretty good. Because people will be pointing out individual pages on your website too. But all of those individual pages are within your website, which makes it a little bit easier to rank your website rather than just that one page.  

#### [0:14:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=870) |  And obviously, within a website, everything is

linked well together. So you have a clean navigation. You have a way for crawlers to go from one page to the other to kind of explore things from there. So I would say, if you really need to just create one page and put that online, then go for it. That's fine. But if you have a way of integrating this within your existing website, I generally prefer to have fewer web sites rather than a lot more.  

#### [0:15:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=900) |  We had a manual action for pure

spam because of how our domain was used before we purchased it. We purged all the backlinks, fixed any issues, sent two review requests spaced two weeks apart over the past month. But we're still waiting for a response. How can we follow up and have the manual action checked? It feels like it's never going to be resolved. So waiting for the reconsideration request to come back is really the right approach there. There's no way to bump yourself up to the top of the queue.  

#### [0:15:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=930) |  Sometimes things get a little bit packed

in the review queue, and essentially you just need to wait until all of those are resolved. And then they'll get to your site eventually. Resubmitting a site with another reconsideration request won't push it higher, and it won't push it lower. It will essentially keep that initial reconsideration request in the queue, and it will get processed when it comes.  

#### [0:16:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=960) |  With regards to pure spam, that's something

that wouldn't be related to backlinks. That would purely be related to the actual content. One of the things that the reconsideration team does look out for, though, is that there's actually a website live on that website there. So if you go and just delete your whole website, and then do a reconsideration request for a pure spam issue, then the reconsideration team will look at it  

#### [0:16:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=990) |  and say, well, there's nothing here that

we would rank. Therefore, it doesn't really make sense for us to process this reconsideration request because there's nothing here now. If we remove the manual action there still won't be anything here, so there's nothing they could change there. So if you're just starting, or if you're cleaning up a pure spam manual action, make sure that you have normal web content on the website so that the reconsideration team can look at it and say, well, there is good stuff here.  

#### [0:17:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1020) |  We need to make sure that it's

indexable. Another thing maybe to mention, especially with the pure spam manual action, is that it removes things completely from indexing as well. So if the pure spam manual action has been in place for a while, then our indexing system will have to start get rolling again for your website, which sometimes takes a few weeks as well. So when you get the reconsideration request back, and it says, all is clear, you're ready to go,  

#### [0:17:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1050) |  that doesn't mean your website will suddenly

pop back into search results. That means our systems can then start working on it. And maybe a couple of days later, maybe a week or so later, that content will start to appear in Search Console and in our indexing system. That sometimes just takes a little bit longer to get back into play again compared to some of the other manual actions. Or for example, if a website was hacked temporarily and got the pure spam manual action for a really short time,  

#### [0:18:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1080) |  then things would still be in our

index, more or less, and we can just reactivate them. But if it's been out for a longer time, it takes a little bit longer. MAX KRAMER: Thank you for the clarification, John. JOHN MUELLER: Sure. MAX KRAMER: That was actually my question. And thank you very much for following up with the last point on it's going to take a little while before things start to go back into the index. Given that we do have some pretty awful backlinks still in there and our manual action was actually approved today,  

#### [0:18:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1110) |  which we can definitely celebrate about-- JOHN

MUELLER: Woo-hoo. MAX KRAMER: --is there anything you'd recommend that we should look at additionally for improving indexing, given that there are crappy backlinks out there and references? JOHN MUELLER: I would double check what kind of bad backlinks they are. If it's something where the previous webmaster really did some shady link building, then that's something that you might just want to disavow on a domain basis. Maybe take the biggest ones or the ones  

#### [0:19:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1140) |  that look the worst to you and

just disavow those. But in general, that's something where, purely from an indexing point of view, that wouldn't be playing a role there. MAX KRAMER: OK, brilliant. Thanks a lot for the clarity. And then in that case, I guess just a waiting game. I'll wait a couple of days a week or so? And hopefully see some results? JOHN MUELLER: Yeah. What you'll probably see is that the crawling side takes a little bit to start rolling again.  

#### [0:19:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1170) |  And then after things have been crawled,

then the indexing side can happen again. So here, watching the server logs, if you have access, then you should see at some point, well, crawling is starting. And then you know it's not going to be too much longer until indexing is back. MAX KRAMER: Cool. Thank you very much. JOHN MUELLER: Sure. A year ago we disavowed a long list of links we thought were hurting our site. Within that list, there was a list of about 15 to 20 links  

#### [0:20:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1200) |  we were not so sure about and

disavowed them anyway. After reviewing our disavow file we'd like to check if those links help our site or not. Should we remove them from the disavow file all at once, one by one, every couple of weeks? Or is this not a good approach at all? How should we go about it? So from my point of view, if you're unsure about certain links in your backlink profile and you know these were not created in a bad way,  

#### [0:20:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1230) |  then I don't think you need to

disavow them. I'd really only try to disavow things where you're really sure that actually there's something bad behind that that you aren't able to clean up, or if you're in a situation where you're losing sleep because you don't know what Google systems are doing. And you just want to make sure that it doesn't misinterpret something. Then, of course, disavow is probably not necessary, but it's good for peace of mind. So from my point of view, if you don't  

#### [0:21:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1260) |  want to disavow those anymore, I would

just remove them from the file and move on. My guess is with 15 to 20 links that you're not sure are really terrible-- my guess is those wouldn't play a big role in your site's ranking. So it's probably not something where you'd see a jump in traffic suddenly because those links are counted again, but rather maybe a subtle change over the long run.  

#### [0:21:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1290) |  But if you're unsure about these, and

you're talking about a handful, 20 links, something like that, then I wouldn't worry about it. I would just remove them from the disavow file and let them be processed normally. ITALY: Hi, John. This is actually my question, and I wanted to follow up on your answer. JOHN MUELLER: OK. ITALY: So if those links were actually spammy links and we removed them from the disavow file all at once,  

#### [0:22:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1320) |  will that affect badly on our website?

JOHN MUELLER: Essentially, we would try to process those links when we try to recrawl those pages, and we would just treat them as normal links to your site. And our algorithms do have some protections in there where, when we recognize that something is spammy, we try to ignore it. So that's something, ideally, they would help your side a little bit. In the worst case, they would probably just be ignored.  

#### [0:22:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1350) |  ITALY: OK, I understand. Great. Thanks. JOHN

MUELLER: Sure. Due to a technical problem, we had a period of seven to eight weeks a duplicate content issue. Pages from our main site were one to one served on our verticals without a canonical description. We fixed the issue recently. And on the wrong pages the verticals returned 404s. Do we have to expect a penalty for this, or are we already seeing the results of such a penalty?  

#### [0:23:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1380) |  So maybe the first one before we

move on to the other questions. No, this would not result in a penalty. This would not result in a manual action from the web spam team. It's also something that, algorithmically, when our algorithms look at your site, they're not going to say this is a spammy site just because you have some duplicate content, kind of like where you're mirroring things on a per-page basis. This kind of duplicate content is very common.  

#### [0:23:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1410) |  A lot of sites, for example, have

the same content on the dub dub dub version as well as on the non dub dub dub version. Technically, that's duplicate content. In practice, that's kind of the way the internet works. It's no big deal. So definitely from a manual action point of view, there is no manual action for duplicate content. The only time a manual action would come into play with duplicate content is if your website is essentially scraping a bunch of other websites.  

#### [0:24:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1440) |  And it's not so much a matter

that you have some duplicate content across your website, but rather your whole website is duplicated from other websites. There is nothing valuable for us to index on a website like that, so that would be a place where the web spam team would say, maybe we need to take a manual action. But this kind of thing where individual pages are just mirrored, no problem. Technically, it's good to clean that up. It improves indexing a little bit.  

#### [0:24:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1470) |  It makes it easier for us to

focus on the right pages, but it's not that you're going to see a big negative drop in search because of that. The verticals in question show over two million pages indexed in Search Console. Is it a good idea to speed up the 404 recognition process with an upload of a sitemap for the verticals including explicitly the wrong URLs? Theoretically, you can do that. In practice, I don't think that would make a big difference.  

#### [0:25:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1500) |  So what is probably happening in a

case like this is we would index these pages individually. And when it comes to showing them in search we would say, well, we have a few copies of the content. We will pick one of these pages and just show that. So we're already folding them together when it comes to the search results. So it's not that you need to remove these from search immediately. With regards to having them be processed a little bit faster,  

#### [0:25:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1530) |  having the graphs go back to a

more normal situation a little bit faster, that's something where you could do something like this. Maybe a sitemap file of pages that are now 404. Or if you have redirects in place, so it would link to those redirects from the site map file, that could help there. I think in this particular case, if you're not worried about showing something wrong in the search results, I wouldn't necessarily worry about it.  

#### [0:26:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1560) |  So you can probably trigger these pages

to show in search if you do a very tricky site query, but for normal queries we would probably pick your primary pages. NEERAI PANDEY: Hi, John. JOHN MUELLER: Hi. NEERAI PANDEY: So John, actually regarding sitemap [INAUDIBLE] sitemap, I was thinking something in my mind. So how would Google behave if I have a list of the 404  

#### [0:26:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1590) |  pages in XML sitemap? So one page

it goes and crawls. Finds 404. Does it mean that Google is not interested to further go to other pages because they all are 404 [INAUDIBLE]?? JOHN MUELLER: No. Just because you linked to 404 pages in sitemap file doesn't mean the sitemap file is bad. So we would still use the sitemap file. The important part is the last modification date there  

#### [0:27:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1620) |  which tells us that something has changed

on these pages. We would still try to prioritize crawling of those URLs. And so in a case like this where you have sitemap files linking at 404 pages, we would just try to crawl those, and at some point, we would say, well, it returned a 404. We don't need to index this page anymore. And that's fine. I suspect we would show this in Search Console as well and say you submitted pages that returned 404.  

#### [0:27:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1650) |  But if you're doing it on purpose

you know that that's going to happen. So it's not going to cause any problems otherwise. NEERAI PANDEY: Cool. JOHN MUELLER: All right. Considering that Google is still at its early stages to understand natural language written for non-English content, do you think it's a good idea to use more structured data on foreign language sites? This is, of course, to make it easier for the indexer to understand the content of the page quickly.  

#### [0:28:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1680) |  That's a unique question. I don't think

I've seen that before. In general, we do try to understand the language on a page a little bit better. Sometimes it works better in some languages, and sometimes it works really terribly in other languages. But it is something that we work on. With regards to using structured data to explain the entities on a page,  

#### [0:28:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1710) |  I think that's generally fine. And it

definitely wouldn't have any negative effects. I suspect for the languages where we have trouble understanding the language, it wouldn't play that much of a role, though. Because if we can't understand the entities within the queries, then we would have trouble mapping that to content on your pages. And in those cases, we would fall back to the situation-- well, we understand that these are sentences  

#### [0:29:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1740) |  or that these are words, and these

words are on that page. And that page makes sense to rank for these kind of queries with those words. So we would try to do it more on a per-word basis. So I think giving us more structured data on pages is generally a good practice anyway, but you're probably not going to see a big jump in relevant search results for your site  

#### [0:29:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1770) |  by implementing structured data in a language

that is hard for us to properly understand. XAVIER NAUDEAU: Hi, John. Can you hear me? JOHN S: Hi, John. I have a quick follow-up question. JOHN MUELLER: Sure. Wow. One at a time. JOHN S: Xavier, go ahead. XAVIER NAUDEAU: Yeah, thank you. I just wanted to jump on this [INAUDIBLE] other thing because, as you may know, we can't pass everything we want in a company or its clients to make them implement stricter data and other stuff because of a priority with IT, et cetera.  

![](https://i.ytimg.com/vi/Vej7f43fiyM/maxres2.jpg)



#### [0:30:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1800) |  As of today, the official communication was

implemented. It will be better for Google, but at some point, will we have, some day in the future, an official announcement saying that Google is taking into account this stricter data to enhance, in a way, the ranking? Because as of now we don't have enough arguments to tell people, yeah, you need to improve on this because reasons? Because maybe Google will use it?  

#### [0:30:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1830) |  And do you see where I'm going?

JOHN MUELLER: Yeah. XAVIER NAUDEAU: We want to push that stuff out to you, but you need to [INAUDIBLE] this one. Thank you. JOHN MUELLER: Yeah. I think that's tricky. So the clear thing at the moment is some of the search results feature the way that we show things, they require certain structured data. And I think for those, the argument is a little bit clear in that, if you want your search results  

#### [0:31:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1860) |  to be shown in this fancy way,

then you need to give us the information that we can show there. So that's, I think, a little bit clearer. It doesn't provide any ranking boost, but at least it's a visible change with the search results. And sometimes having a page being more visible in the search results is useful just the same as ranking a little bit better. With regards to using structured data in general for ranking, I think that's kind of tricky. So on the one hand, we do use structured data  

#### [0:31:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1890) |  to better understand the entities on the

page and to find out where that page is more relevant. But that doesn't mean that, just because people are doing things in a technically correct way on a website, that that page is a better page than it would be otherwise. So we will try to use that to show it in more relevant search results that would perhaps bring more users to your pages that actually match the topics of your pages.  

#### [0:32:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1920) |  But it doesn't mean that we would

show it to more users or that it would rank better. So that's something where I don't really see that changing in the future. That's something that goes back to-- I don't know-- way in the beginning where people would ask, does it valid HTML? Is valid HTML a ranking factor because clearly a page that has valid HTML, they spent more time on that page? From our point of view, well, they  

#### [0:32:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1950) |  might have spent more time doing the

things technically correct, but does that mean that the page is actually a better search result? Is that really something that provides more value to the user? Or is it just that someone was a little bit smarter in creating that page, and the actual content that people would read is not actually that much better? So that's the tricky balance there. And I don't see that changing in the sense that we would suddenly say, any page with structured data will have a ranking boost, because structured data isn't  

#### [0:33:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=1980) |  visible. So even that would be something

we would be ranking things higher for technical reasons that users would not even be able to appreciate. XAVIER NAUDEAU: OK, thank you. JOHN S: Hey, John. A quick follow-up question there. Do you see, in the near medium future, Google moving away from structured data because it's better able to understand natural language?  

#### [0:33:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2010) |  JOHN MUELLER: To some part, I could

see it easier for us to extract the entities of a page over time. We already do that. It's something that I think is-- I don't know-- probably still in early stages. But there are lots of things where we still need to kind of rely on input from webmasters to be able to highlight that properly.  

#### [0:34:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2040) |  So things like the rich search results

where individual elements of a page need to be extracted, and we need to be able to understand those properly, and to say, well, this is a review count. And this is the maximum star rating, and they have four out of five, and so many people reviewed something. All of these kind of details are pretty tricky to extract automatically even if you have a really good system. So you see some of that with the Data Highlighter tool, where  

#### [0:34:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2070) |  we do try to extract things based

on patterns on your website. And that's something where you can try that out with the Data Highlighter in Search Console to see how well things can be followed even if you clearly define which piece of element is which part on the page. So I think part of that will definitely remain, and certainly in the near future. So if anything, when talking with the engineers,  

#### [0:35:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2100) |  they come up with more ways to

use structured data rather than fewer. The one thing that I have seen, which I thought was pretty cool, is that some of the engineers have been looking into finding ways to recognize when a site could be using structured data to highlight things in a way that we could understand it a little bit better. So for example, if we can recognize  

#### [0:35:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2130) |  that there are events on your pages,

even if we can't extract those events properly ourselves, we could theoretically send you an email in Search Console and say, hey, it looks like you have events on your website. If you want those events to be shown within the events features and search, maybe structured data would be an idea to help out there. So that's probably more the near future and the medium future. I don't see it going away there. In the really long-term future, I  

#### [0:36:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2160) |  have no idea how that will evolve.

JOHN S: Got it. Thank you. That's helpful. JOHN MUELLER: All right. Is there anything you can advise as crucial for single-page application sites? Which [? 308 ?] code should we use to not lose anything from a link's equity, and how long should we keep that redirection? So I guess those are two different questions. Single-page applications, we have tons of documentation on that. So these are essentially JavaScript-based sites  

#### [0:36:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2190) |  where you have one HTML file. And

by JavaScript, you're essentially pulling in different content from the server and creating a different site under individual URLs. I think the main thing I would focus on there is to make sure that you have clear index of all URLs. That seems to be the main thing that people run into when they started looking into this problem. And apart from that, for JavaScript rendering, we have a ton of information in our developer documentation.  

#### [0:37:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2220) |  So I would recommend going there. In

particular, Martin has put together a bunch of videos as well on this topic. So lots of information on how to make single-page application sites work well in Search. I think it's still one of those areas that is bleeding edge. So if you're unsure about all of this, if you're not that technically versed with JavaScript and all of these things, then it might be something  

#### [0:37:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2250) |  to try to get some help for

from people who have had a little bit of experience here just to make sure that you're not running into any pitfalls when you're creating things. Once you're sure that your setup works well in Search, then that's something where it's a little bit easier to just say, well, it's working. My content is being indexed. I can focus on the content and not so much on the technical details. Which [? 308 ?] code should we use to not lose anything from a linked equity?  

#### [0:38:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2280) |  So there, essentially, we try to differentiate

between two redirects, the permanent and temporary ones. Permanent redirects really tell us that everything should be focused on the destination page, which means that we forward all of the signals to the destination page. A temporary one tells us we shouldn't trust this redirect to remain there forever. It's temporary, so we should focus on the source page instead. And then all of the link signals are focused on the source page.  

#### [0:38:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2310) |  It's not that you're losing anything. It's

just, do you want this page, or do you want this page to have those signals? That's the decision that we need to make there. So use the right redirect type for the type of issue that you have. Do link clicks from Google My Business pages count as organic links or direct clicks? This sounds a bit like an analytics question. I don't really know from an analytics side.  

#### [0:39:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2340) |  Within Search. XAVIER NAUDEAU: It's organic. JOHN

MUELLER: OK. XAVIER NAUDEAU: By the way. JOHN MUELLER: [INAUDIBLE] organic. So if we're showing that map-- what is it? The local results one-box thing-- with a link to your website there, then that would count as an impression. And if someone clicks on the link to your website, that would count as a click in Search Console. So that would be fine. Within the Maps UI, if you're in maps and you search, and someone clicks on your website,  

#### [0:39:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2370) |  we would not count that in the

Search Console. XAVIER NAUDEAU: What you could do, you can add to UTMs for Google Analytics to track it as with the My Business traffic. JOHN MUELLER: Yeah, you can add the UTM parameters there. And I suspect you can see that in Google Analytics. It's not guaranteed that you would see that in Search Console. The main reason there is that, in Search Console, we would try to show the canonical page, and the canonical page is probably  

#### [0:40:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2400) |  the one without the parameters. So it's

hit and miss. In talking with the Google My Business team here, they're not so sure which direction they want to take there or which standpoint they think is the right one, so I'm not sure how that will remain in Search Console. In Analytics, I suspect you'll still be able to see that. And then I think the question is also with regards to Google Ads.  

#### [0:40:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2430) |  Would those count as clicks and impressions

as well? Those would not count in Search Console. So in Analytics, probably. In Search Console, they wouldn't count because they're not a part of the organic search results. How important is mobile page speed as a ranking factor? So we did a bunch of stuff, and we improved our score. So should we be ranking higher, essentially?  

#### [0:41:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2460) |  We don't have a measure for the

importance of individual ranking factors, so that's the main thing here. With regards to the meta question of, is it even worthwhile for us to work on speed if we can't quantify exactly what the ranking effect is? I think it makes sense to focus on speed regardless of any ranking factor. The main reason being that when people come to your site,  

#### [0:41:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2490) |  and your site is really slow, they're

going to leave again. If you care about the people that make it to your site, then make sure that they have a good experience on your site. And usually speed plays a big role in that. And there are different ways of measuring speed. There are different metrics. A whole bunch of different three-letter acronyms which I can't remember. Martin knows most of them, but there are so many different variations, especially with regards  

#### [0:42:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2520) |  to the different page types. So if

there's something where you want people to enter something, then maybe you'd look at something different than if you want people just to see your content. So lots of different metrics out there. I would try to find the metrics that are important for you, for your site, for your pages. I would also use the different testing tools to find the low hanging fruit that you can improve fairly quickly. Pretty much every site has some low hanging fruit that you can improve on, which make it easier  

#### [0:42:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2550) |  for you to improve your score across

the board. With regards to the individual testing tools, like Lighthouse or PageSpeed Insights, the score there is mostly a representation of we look at different factors, and we came up with this score. It doesn't mean that this is a target where you should aim to be 100 out of 100 all the time, or that you'll have a clear ranking bonus if you have everything set up.  

#### [0:43:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2580) |  When it comes to ranking, we look

at a mix of metrics. On the one hand, these theoretical metrics that come out of the testing tools, but also at field data from things like the Chrome User Experience Report where we see what people are actually seeing when they come to your website. So it's not that you need to game the PageSpeed Insight score and get 100 out of 100 there. But rather, essentially, if you wanted to game the score you would do it by making a really fast website, which is what we're trying to encourage people to do.  

#### [0:43:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2610) |  So summarizing all of that, there is

no specific kind of weight of individual ranking factors that we can give, so I can't tell you how important this is. It does make sense to focus on, even regardless of the ranking elements, and we do use it in ranking. So it is something where we do have an effect in the ranking side specifically around speed.  

#### [0:44:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2640) |  How often do you check the overall

quality of content on a website as a whole? Is it monthly, or more often than that, or is it only when the core updates happen? We essentially have different algorithms that are running all the time, and they look at all kinds of signals that we can collect about your content. And they try to come up with a notion of quality  

#### [0:44:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2670) |  and a notion of relevance for individual

queries for the website on a running basis. So it's not that there is a one-time run where we go across all websites on the web, and look at all of the content, and evaluate the quality. I don't think that would be feasible. On the one hand, we can't look at a whole website at one time. We have to crawl individually and spread that load out over a longer period of time just purely from a technical point of view.  

![](https://i.ytimg.com/vi/Vej7f43fiyM/maxres3.jpg)



#### [0:45:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2700) |  And with regards to the core updates

these are essentially just different ways of us evaluating the quality of a website. So it's not that we would go and look at the website again, then, but rather we have collected all of these signals about this website, about how we think we should treat it. And then the core algorithm updates, they essentially say, well, we use slightly different numbers when calculating the total relevance for individual queries. So it's not that we re-evaluate the website, then,  

#### [0:45:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2730) |  but rather that we recalculate the scores.

We built an SEO-optimized site with an SSL certificate and a one-second loading time. But it ranks below sites that have no SSL, bad user experience, and take a long time to load. What tips do you offer to rank above them? So these are great things. These are really good technical things to implement. I think having HTTPS is a great thing to do.  

#### [0:46:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2760) |  Especially nowadays, that's kind of expected. A

one-second loading time is fantastic, so that's really awesome. I think, in general, you're on the right path there. In practice, that doesn't mean that your site is suddenly more relevant than other sites out there. So I would still take this as a good foundation to build on, and then work to make sure that your whole website is really fantastic, and ideally that it's really significantly  

#### [0:46:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2790) |  better than the others that are out

there, so that it's clear to our algorithms that we should show your website above the others. And that's something that's not just based on technical details, where you really need to make sure that the content is really significantly better. So if you were, for example, just to take the content from another website and host it on HTTPS with a really fast server, then that doesn't mean that content is suddenly better because it's essentially the same as the other website  

#### [0:47:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2820) |  already has. So what big difference does

it make there? So really take a step back. The technical details are fantastic, but you need to deliver the actual content as well. Is the Google Sandbox real? What's the impact on a new website? So, no, there is no Google Sandbox. I think we've said this tons of times, but these questions come up all the time.  

#### [0:47:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2850) |  It's something where we don't have this

kind of notion that a new website should be blocked from being shown in Search until it has reached, I don't know, a maturity of half a year or a year or something like that. We don't have that kind of a notion built into our algorithms. However, there are certain things that could look like that. So in particular if you're a completely new website, then we don't have a lot of signals for your website for the content that you have there.  

#### [0:48:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2880) |  So essentially what happens is our algorithms

say, well, we don't really know that much about this website, so we're going to guess. And in the beginning, that guess might be positive. That's something that some folks externally have framed as a honeymoon period where Google really likes new web sites. It could also be that that initial guess is a little bit too low, where we say, well, we're not sure about this. So we'll put it kind of in the mid-range.  

#### [0:48:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2910) |  And there's some really good stuff here,

so the mid-range versus the really good stuff is going to be hard competition. So you might be ranking a little bit lower than you would in the long run. And this is the kind of thing where it's not that our algorithms are saying we should, by default, block this website from showing up, but more that we just don't know how we should be showing this website ideally in Search. So we start off with some assumptions, and we see where that settles down over time.  

#### [0:49:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2940) |  So that's probably the effects that you

would see there, where some sites, when they start off, they start off really well because we think, well, this looks really good. And for other sites we don't really know where to place them either, and maybe with that starting point it's a little bit lower than it would be when it settles down for the long run. And obviously the settling down part is not static either. Ideally, you would create a website  

#### [0:49:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=2970) |  and you would grow your web presence.

You would promote it online. You would bring people to your website. People would say, oh, this is fantastic. I've been waiting forever for this. And they would link to it from other sites as well and promote it for you a little bit too. So that's something that's very dynamic as well. It's not that you put a website up once, and then you never touch it again, and nothing on the rest of the web changes for a really long time. The whole web is dynamic, so continuing working on a website  

#### [0:50:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3000) |  to make it better over time is

definitely what I would aim for there. OK. I think we're running low on time, so I thought I'd open it up for you all to see if there's anything on your mind that we should be focusing on. It looks like a bunch of back and forth in the chat. Yes. NEERAI PANDEY: Hi, John. JOHN MUELLER: Hi. NEERAI PANDEY: So I have one question, actually, about M dot  

#### [0:50:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3030) |  site and WWW website. So I know

that Google recommends for a responsible desktop website is already running in this case. We were just going with m-dot website. In this case, John, actually, if I want to go with A/B testing for WWW redirection to new URL then canonical to old one, I have just gone through.  

#### [0:51:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3060) |  But how should I manage m-dot website,

especially when it is mobile-first index, and I have to provide the equivalent version or equivalent behavior in both devices? Or suppose not m-dot but AMP also is available? JOHN MUELLER: Yeah. So I think you're making it really hard with this kind of a setup if you have desktop, m-dot, AMP, and you're doing A/B testing across different URLs.  

#### [0:51:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3090) |  You're creating a whole bunch of URLs

for essentially one piece of content, and that makes it so much harder for us to process properly. So in practice, you would work with the canonicals to focus both for the m-dot, A/B testing versions. You would set the canonical from the m-dot B version to your main version, assuming that the A  

#### [0:52:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3120) |  version is the main version. And the

same thing on your desktop. You would take the desktop B version and canonical to a desktop main version so that we can understand that connection a little bit better. You have the same setup with the AMP pages where you have the AMP page, the canonicals set to the desktop version or the mobile version. But I think with all of this extra complexity of different canonicals and different URLs with the same content, it's something that you  

#### [0:52:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3150) |  have to assume will cause issues in

the sense that our systems might be confused with regards to which URL they need to actually index. So instead of just one URL with a responsive site, we suddenly have six or more URLs that all have the same content, and we have some canonicals and some cross-linking there, then it can easily happen that our systems pick a wrong URL for indexing.  

#### [0:53:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3180) |  In practice, what that means is we'll

index the content on that URL. We'll pick that as the canonical and show that in the search results. If the A/B testing is minimal, then probably that's no big deal. If the A/B testing is significant with significantly different content, then we would index that significantly different content, and that could be reflected in your ranking. With regards to different device types, we do recommend using redirects between the device version.  

#### [0:53:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3210) |  So if a desktop user goes to

the mobile version, they will be redirected to the desktop version. So that's less of an issue. They would still get to the right version. However, if you're tracking things in Search Console, or if you're tracking things in Analytics, and you're monitoring it by URL, then I would expect to see some amount of confusion across those different versions. That's primarily also the reason why we suggest moving away from an m-dot version  

#### [0:54:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3240) |  to just the purely responsive version, because

the less confusing you give us the more likely we'll do what you ask of us. So if we just have one URL to index for that piece of content, then we'll say, sure. We'll index it under that one URL because that's all we know. Whereas if you give us lots of different URLs for the same content with some connections with rel canonicals and maybe redirects in place, then we'll be like, we'll try to figure it out for you. But it's not sure that we'll figure it out  

#### [0:54:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3270) |  in the way that you actually want,

or in the way that is useful for you when you're tracking metrics. NEERAI PANDEY: John, actually, this was already old website. And suddenly we were not able to just move it to responsive. This is why m-dot was still going on. JOHN MUELLER: Yeah. NEERAI PANDEY: And John, it means that if responsive then for all the devices, the A/B testing will be going on. In the same way with m-dot website, how Google  

#### [0:55:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3300) |  would behave if I only want to

do A/B testing for desktop? [INAUDIBLE] JOHN MUELLER: I think with an m-dot version on mobile-first indexing, we would index only the mobile version, so we would not even see this A/B testing on desktop. So that's something that makes it a little bit easier in that you can do the A/B testing purely from a user point of view on desktop, but we would not see that for indexing.  

#### [0:55:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3330) |  NEERAI PANDEY: All right. All right. JOHN

MUELLER: Sure. NEERAI PANDEY: Thanks, John. JOHN MUELLER: All right. Someone else, I think, had a question, but I forgot who that was. No more questions. Oh, my gosh.  

#### [0:56:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3360) |  OK. Let me see if I can

grab a quick one from YouTube. A question about verifying status codes. In the answer, you said if it's a 400 or a 500 error or redirect, then obviously those are things that we wouldn't render. We're planning a website switch that will include some changes of URLs which we're currently ranking. Will redirecting those URLs to new ones help the new ones get discovered,  

#### [0:56:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3390) |  or will Google see the 301 status

code and ignore them? So I think that might have been unclear, or obviously unclear if the question comes up, in the previous Hangout. Essentially, that was specific to rendering the page. So if there is JavaScript content on the page, and that page itself is returning a redirect or an error code, then we would not render the JavaScript content.  

#### [0:57:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3420) |  You could imagine that you have a

user friendly 404 page, and it uses JavaScript to show some content, including some links to other pages. Then we would not kind of use time to render that error page because we already know, well, it's a 404. Any of the content that we find here would not be indexable anyway. So we would not render the page in that case. When it comes to redirects, obviously, we wouldn't render those pages either,  

#### [0:57:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3450) |  but we would follow the redirect. We

don't need to render the JavaScript. We'd just follow the redirect to the new location. So if you're changing URL from old URLs to new URLs, then definitely a 301 redirect is the right approach here. And it doesn't matter if we render the redirecting page or not because, even in the browser that move would be fairly quick. And you would kind of immediately go to the next step in the redirect.  

#### [0:58:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3480) |  You wouldn't even try to render the

HTML that's returned with the redirecting page. I don't think-- most browsers wouldn't even show that. So from that point of view, 301 redirects is definitely the right approach. BARRY SCHWARTZ: John, I have a question about the in-this-video-- JOHN MUELLER: OK. BARRY SCHWARTZ: --or feature. I don't know if you know anything about that. But just to be clear, if you have YouTube videos and you're putting the time stamps in the description, you don't need to apply for that beta form or that schema,  

#### [0:58:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3510) |  right? The scheme is only if you

have your own videos posted on your own site? JOHN MUELLER: I don't know. Good question. Is that unclear in the documentation? BARRY SCHWARTZ: I think it was clear that you don't need to submit the form if you have YouTube videos. JOHN MUELLER: OK. BARRY SCHWARTZ: And so somebody asked me and they think it was not. So I just wanted to clarify that with you. JOHN MUELLER: I don't know. BARRY SCHWARTZ: I'll tweet at you and see if you can look into that. JOHN MUELLER: I think the best approach would be just to prove it.  

#### [0:59:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3540) |  BARRY SCHWARTZ: I don't want to prove

it because I do both. And now it's like, all right. Which one is it pulling from? And it doesn't even pull them anyway from my YouTube videos because I have, like, 50 timestamps. I don't think it would look at [INAUDIBLE].. So the next question is, how many timestamps are too many timestamps? JOHN MUELLER: I don't know. BARRY SCHWARTZ: And since this feature's in beta, when is it going to be released? And then if we do it do we get a DA ranking boost? Supposed to spit that out. JOHN MUELLER: Yeah, I think that might happen. Yes. I don't know. BARRY SCHWARTZ: You don't know? OK, awesome. JOHN MUELLER: Maybe.  

#### [0:59:30](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3570) |  Definitely. BARRY SCHWARTZ: It depends. Got it.

JOHN MUELLER: It depends. It depends. Yes. All right, perfect ending. OK. So we're at time. Thank you all for joining in. Thanks for all of the questions, and all of the discussions here as well. I hope you found this useful, and maybe we'll see each other again in one of the future Hangouts. Wishing all a great week. Bye, everyone. JOHN S: Bye, John.  

#### [1:00:00](https://www.youtube.com/watch?v=Vej7f43fiyM&t=3600) |  Thank you.  

