[![English Google Webmaster Central office-hours from March 20, 2020](https://i.ytimg.com/vi/wX9MKAgOG1E/maxresdefault.jpg)](https://www.youtube.com/watch?v=wX9MKAgOG1E)

## English Google Webmaster Central office-hours from March 20, 2020

This is a recording of the Google Webmaster Central office-hours hangout from March 20, 2020. These sessions are open to anything webmaster related like crawling, indexing, mobile sites, internationalization, duplicate content, Sitemaps, Search Console, pagination, duplicate content, multi-lingual/multi-regional sites, etc. 



Watch out for new sessions, and add your questions at https://www.youtube.com/user/GoogleWebmasterHelp/community



Feel free to join us - we welcome webmasters of all levels!



#### [0:00:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=0) |  JOHN MUELLER: All right, welcome, everyone, to

today's Webmaster Central Office Hours Hangouts. My name is John Mueller. I am a Webmaster Trends Analyst at Google in Zurich usually. Now I'm at home like, I guess, most of you. Part of what we do are this Office Hour Hangouts where folks can join in and ask any question around their website and web search. And we'll try to find some answers for you. A bunch of things were submitted already on YouTube.  

#### [0:00:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=30) |  But if any of you want to

get started with a question of your own or something you submitted there, feel free to jump on in. SAIDUL HOQUE: Hi, John. JOHN MUELLER: Hi. SAIDUL HOQUE: I have a question about [INAUDIBLE] we are a gift [INAUDIBLE] diffent type of [INAUDIBLE]..  

#### [0:01:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=60) |  The last [INAUDIBLE]. Our ranking was good

for the [INAUDIBLE],, but after a [INAUDIBLE],, we believe that our ranking dropped a little bit. And when we checked our computer, [INAUDIBLE] they had [INAUDIBLE] on their category page. And the content was-- if the content doesn't have the site information. It has information like the [INAUDIBLE] and what type of [INAUDIBLE] you should buy.  

#### [0:01:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=90) |  So [INAUDIBLE] it was a blog post

they put on their category [INAUDIBLE] shop. On the other hand, we put [INAUDIBLE] depiction like [INAUDIBLE] and what is the last thing you ordered, and [INAUDIBLE] to provide this information. So now, how does Google decide which content is useful? Is there a way? JOHN MUELLER: It's really hard to understand you from the audio.  

#### [0:02:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=120) |  But I think what you're asking is

you have category pages. And there's a lot of extra content on those category pages. And you're wondering is that good or not? Is that roughly it? SAIDUL HOQUE: Yes. JOHN MUELLER: OK, so in general, having some amount of text on category pages is always useful because it helps us to understand those pages a little bit better.  

#### [0:02:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=150) |  But the important part is also that

we can still recognize that this is a category page of an e-commerce site, and not a blog post, or like a Wikipedia article on the type of products that they sell. So with that in mind, I would try to put useful texts that makes sense for a category page on the category page. And if you want to write background information, if you want to include the full copy of the terms of service,  

#### [0:03:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=180) |  and things like that, I would put

that on a separate page. That makes two things a little bit easier for us. On the one hand, it's a lot easier for us to determine the intent of the page. So is this an informational page. When a user is looking for information, should we send them to this page? Or is this primarily a shopping page where someone is trying to buy products?  

#### [0:03:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=210) |  And we should send them to this

page when they're trying to buy a product. So that's something where you need to make up your mind, what kind of page you want it to be. Because it's not that you can just do all of these things on one page and Google will think it's perfect for all kinds of needs. But rather, what we hope to do is look at this page and say, well, we don't know what the best use of this page is because there are so many different aspects here. We have to guess.  

#### [0:04:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=240) |  So the clearer you can really make

a category page be what you want that page to be, address the needs of the users that you think they will have, or that you want to cover, then the clearer we can follow that and say, oh, this is a really strong category page for people within shopping intent. We should send them to this page when they want to buy a product. SAIDUL HOQUE: Thank you. JOHN MUELLER: Sure, yeah, Robin?  

#### [0:04:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=270) |  RAJEEV LEKHWAR I was going to ask

you a quick question. JOHN MUELLER: Sure. RAJEEV LEKHWAR: So, John, actually I'm working on a B2B client. And he's into the business of sales intelligence tools. So my question is that in such a tight B2B space, is it good to do FAQ schema implementation on their FAQ pages? Because big guns like Salesforce and Zoho, even they are not making use of that schema.  

#### [0:05:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=300) |  JOHN MUELLER: Well, I think if other

people aren't using it, that gives you more opportunity. So I wouldn't say that's a bad thing. If the competitors aren't using the new features yet. But I would think about what content you want to provide there in the FAQ sections to make sure it's really frequently asked questions, and that it's specific to the individual pages, not that you just copy all of the questions to all of the pages on your website.  

#### [0:05:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=330) |  But I wouldn't assume that just because

a big website is doing something or not doing something, that they're doing that on purpose. Sometimes they just don't have time to fix things or change things. RAJEEV LEKHWAR: Well, thanks, John. Stay safe. JOHN MUELLER: Sure, you, too. All right, go for it, Robin. ROBIN LESPAGNARD: Hi, John. First, I'd like to introduce myself. My name is Robin Lespagnard.  

#### [0:06:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=360) |  I'm the founder and CEO of Selectos,

a product review website in French, and proud employer of 12 full-time employees. So I'm going to read out my question out loud, I guess. Hi, John. We sent a reconsideration request for our website on the 16th of March. Should we be expecting a longer review period than the usual few days to two weeks due to the situation, right, with the coronavirus? And also, if I may ask, does Google sometimes  

#### [0:06:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=390) |  send a warning before applying a manual

action? Because in the case of our websites, we instantly lost 90% of our traffic and revenue, putting our company and its fellow employees at risk of bankruptcy. We realize we made a mistake, which we quickly corrected after receiving penalty. But a simple warning with something like 15 or 30 days notice would have done the job, too. [INAUDIBLE]  

#### [0:07:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=420) |  JOHN MUELLER: OK, so I took a

quick look at the site just beforehand. And usually, with regards to reconsideration requests, it does take a lot longer than just a couple days, especially the linking reconsideration requests I've seen. They sometimes do take like up to a month or so.  

#### [0:07:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=450) |  So that's something where at least at

the moment, they're quite overloaded with regards to these kind of requests. I can double-check with the team to see if there's something that they can do here, if this is something where there's like a clear case where they can just say, oh, we don't need to spend a lot of time on this. This is actually OK now. With regards to sending a warning, that's something we do discuss from time to time. And I don't know if that's something that will change.  

#### [0:08:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=480) |  The primary reason why we tend not

to send a warning is that when we run into this kind of a situation where the web spam team notices that there's a problem, then usually that's something that's affecting the normal search results already. So that's something where users are seeing bad results, and we know that users are seeing bad results, then that's something why we want to take action as quickly as possible and to resolve that. So for the most part, that's always kind of tricky to say  

#### [0:08:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=510) |  we would send a warning when we

know that it's currently in a bad state. One other thing maybe, especially with regards to linking reconsideration requests, I think that's the issue that you had there. One of the tricky parts there is that sometimes a site's ranking is partially due to some of these link schemes  

#### [0:09:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=540) |  that they were doing. So by fixing

those link schemes and removing them, or disavowing them, then you're removing some of that kind of artificial support that was for that website, as well. So it won't necessarily be the case that, when the manual action is resolved, that it'll pop back up right at the same place as it was before. Just because you've kind of removed that artificial aspect, then it's like, well, you're in the natural place again, which might be a little bit lower than before.  

#### [0:09:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=570) |  It depends a little bit on how

the algorithms looked at that in the past. But I'll definitely take a look with the web spam team to see if there is something that we can do there. It's kind of tricky because I know they also have a lot of work to do. And if I send them everyone that contacts me directly, then suddenly I'm just the person that slows everything down, rather than kind of the process. ROBIN LESPAGNARD: Thank you.  

#### [0:10:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=600) |  But I thought, could you maybe explain

to people here, to SEOs, the whole process, from start to finish, of a manual penalty? Because when I checked-- we looked at all the information on Google Webmasters' thing. And most of it, the dates of the days of [INAUDIBLE],, it seems like there is no new updates. Like has the process evolved in some ways? JOHN MUELLER: The manual review process  

#### [0:10:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=630) |  is essentially the same, because people have

to take a look at it manually. The guidelines have been fairly stable for quite some time now as well. I think what will be happening a little bit more over time is that we can automate more of this. We already do a lot of this with some of our linking algorithms, where we algorithmically determine these are unnatural links, and we just ignore them.  

#### [0:11:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=660) |  So it's not the case that someone

manually has to look at all of the links and say, oh, these are bad, these are OK, but rather that we do that automatically. And I think that's the direction we'll probably continue to go, that it's less and less people having to manually take action, and then you have to fix something and send like a letter back saying I fixed it, and then someone looks at it again kind of thing. But more that we can automatically recognize problems, ignore them ideally.  

#### [0:11:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=690) |  If we can ignore them, then, on

the one hand, it makes it harder for you, because you don't know what is happening. But on the other hand, it also doesn't harm your site. So if you accidentally do kind of these things, or if you did these things in the past based on bad advice, maybe, then if we ignore them, then you're not causing your site any harm. But you're also not having any profit from kind of those unnatural things.  

#### [0:12:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=720) |  ROBIN LESPAGNARD: And I'm curious. For example,

in our case, or in another case, does the manual reviewer check the website out, looks at the company, maybe, behind the website? JOHN MUELLER: Sometimes, yeah, sometimes. ROBIN LESPAGNARD: OK. JOHN MUELLER: I mean, it's something where, because these are manual reviews, we can spend a little bit more time. But it's also, the web is big and we have a limited time.  

#### [0:12:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=750) |  So what usually happens is someone will

look at the bigger picture of the website and try to determine if there's really a strong pattern of unnatural issues here, which could be all kinds of things. With links, it's always a bit tricky. That's something where the manual reviewer has to double-check a little bit more and see, is this really something that probably the website did by themselves? Or is this something maybe a competitor did, or someone is trying to harm them and frame them by doing these links?  

#### [0:13:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=780) |  And if we can recognize that it's

not in their control, we'll try to find a way to just ignore those links. But they do spend a little bit more time to look at the website, as well. ROBIN LESPAGNARD: And do you have multiple teams in multiple languages, or just one big team in the US? JOHN MUELLER: We do have multiple people in multiple locations for multiple languages, just because sometimes websites are hard to understand  

#### [0:13:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=810) |  in different languages. And just looking at

kind of the number of links makes it really hard to determine is this kind of unnatural or is this just-- I don't know-- maybe one language that they don't speak, and all of the links look like this because that's a very common word. I don't know. So that's something where we do have multiple teams in different locations. On the one hand, that makes it a little bit fairer with regards to international websites,  

#### [0:14:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=840) |  but that also means, for some languages

and some locations, we might not have as many people as for other locations. So that sometimes slows things down with the reconsideration request. ROBIN LESPAGNARD: And do you have some kind of minimum reasonable time that you expect the website owner to take before sending his request? JOHN MUELLER: No. No. That's something where, if we send a manual action  

![](https://i.ytimg.com/vi/wX9MKAgOG1E/maxres1.jpg)



#### [0:14:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=870) |  and you fix it within half an

hour, that's fine. ROBIN LESPAGNARD: OK, so that was a myth. Cause many people told me, no, no, take your time. Take your time. JOHN MUELLER: No. I think, especially with links, it's more important that you really clean up the bigger picture. And sometimes that does take a bit more time. But there are different issues which you can just fix. If you have a plugin on your website that causes the problem, then you can just remove that plugin, and suddenly everything is fixed.  

#### [0:15:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=900) |  ROBIN LESPAGNARD: OK, and if you want

to stop me, please stop me, but I have some more questions. JOHN MUELLER: Sure. ROBIN LESPAGNARD: Once you receive the request, are there different priority queues? JOHN MUELLER: No, I don't think so. ROBIN LESPAGNARD: OK. JOHN MUELLER: No. ROBIN LESPAGNARD: And how does the team judge that a website has done a sufficiently good job at fixing the problem? Because maybe sometimes the problem is fixed like 90%, but you judge it to be OK, good enough.  

#### [0:15:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=930) |  JOHN MUELLER: Yeah, especially with links that's

really tricky, because you can always find something bad on the internet if you look far enough. So that is something where the team tries to see if a significant amount of the issue was resolved. And if that's OK, then that's usually fine. It's a bit tricky, like I mentioned, with links. And different people might look at it in different ways. So what I have seen happen is that someone  

#### [0:16:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=960) |  reviews a reconsideration request and says, oh,

it's not enough. And then the webmaster sends kind of a message back and saying, it was like everything I could find. And then someone else takes a look and says, oh, yeah, it's actually OK. ROBIN LESPAGNARD: OK. JOHN MUELLER: But I mean, that shouldn't be the usual case, but that's kind of something that is possible whenever there is kind of a manual review process. ROBIN LESPAGNARD: And if the problem has [? not ?] been fixed sufficiently, does the team voluntarily  

#### [0:16:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=990) |  wait a longer time before taking another

look at the website? JOHN MUELLER: No. No. ROBIN LESPAGNARD: OK, so that's another myth too. OK. JOHN MUELLER: That's another myth. ROBIN LESPAGNARD: Mythbusting today. JOHN MUELLER: Yeah, the one thing that can happen is if we see that a website goes back and forth, then that's something where the web spam team will say, OK, you're just wasting our time. Like if you fix the problem, do the reconsideration request, and then a couple weeks later you have the same problem again, then a little bit of back and forth,  

#### [0:17:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1020) |  and the web spam team will say,

OK, we will take a look in a couple of months when you've decided what you want to do. ROBIN LESPAGNARD: And last question now then, after such a review, is the website put in a check regularly to see if no further problem happens list? JOHN MUELLER: No. No. ROBIN LESPAGNARD: All right. JOHN MUELLER: If it's fixed, it's fixed. There is nothing that hangs around. ROBIN LESPAGNARD: Well, thank you very much. I hope this is useful to other SEOs. We busted a few myths here. Thank you for the question. JOHN MUELLER: Yeah, no, thanks. I think it's always frustrating and confusing the first time  

#### [0:17:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1050) |  you have some manual action like this.

So I totally understand all these questions. OK, let me run through some of the questions that were submitted so that we can try to catch up with some of that, too. Is there any image ranking advantage in using the same image in different blog posts when talking about the same topic? For example, every time I post about Parmesan cheese, I use the same image instead of a different image.  

#### [0:18:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1080) |  Could it help that image to rank

better? No, that doesn't change anything for the images, because images don't rank on their own. They always rank together with their landing page. So it's always a matter of this image on that landing page, and the landing page has more information, more context around that image. So it's not something where the image alone would be ranking in the search results. RICCARDO CAMPACI: That was my question.  

#### [0:18:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1110) |  JOHN MUELLER: Great, cool. How can we

use the Mobile-Friendly Test for web pages that are under development? It's an extremely useful tool. But it doesn't work when there's a robots.txt disallow command. We're afraid of opening the dev part of the site, because real Google bot can jump into this hole, too. So there's a guide in our developer documentation on how to setup a proxy for debugging locally  

#### [0:19:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1140) |  hosted or firewalled pages. It's on a

page called Debug Your Pages. I guess that's a good title, which is mostly about JavaScript sites. And essentially, what you do is you setup a local proxy that listens to a specific port. And then you assign a URL to that location. And that way, you can test, essentially, locally hosted pages and try them out there.  

#### [0:19:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1170) |  So that should work for the Mobile-Friendly

test. That should work for anything that you use kind of the URL inspection tools for. One thing to keep in mind with the URL inspection tools is that you would need to add this host to your Search Console account first. So there's a little bit more back and forth involved there if you need to use the URL inspection tool, but mobile-friendly, rich results test, structured data  

#### [0:20:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1200) |  test, all of those should work right

away. One of our clients creates paginated pages with similar or synonym names, in H1 title internal linking. What's your opinion about this? Can this help to avoid keyword cannibalization and help us to rank for different keywords for the same category? Because theoretically and practically, it seems to be a great strategy, as I've seen, that our page six of a category got good clicks because maybe  

#### [0:20:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1230) |  nobody else is using that specific combination,

and we have it on the H1. So in general, you can do whatever you want on your category pages, on your paginated pages. However, because of the way that paginated pages tend to be linked within a website, where you link to the first page of your paginated set, and then you link to the next page from there, and kind of the third page from there,  

#### [0:21:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1260) |  and you have this chain of linking,

we kind of see the first page of a paginated set as being the most important one. And that's the one we tend to show in the search results when someone is looking for this category of products. So if you spend your time kind of trying to improve page six of a category pagination set, then probably that's a lot of time  

#### [0:21:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1290) |  that's spent on things that are very

little visibility in search. So my general recommendation there would be to say, instead of trying to improve this kind of like far-off and less relevant page within your website, try to focus on the more relevant pages on your website, and make those a lot stronger. So instead of going off and doing these super long-tailed things where you get a little bit of traffic, and you might be ranking well  

#### [0:22:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1320) |  for those keywords, but if nobody is

searching for them, that's not very useful. Instead of focusing on those really long-tail things, focus a little bit more on making the core of your website much stronger. Because by making the core of your website stronger, everything around that will be a little bit stronger, as well. So that's kind of the direction I would head. It's not that you can't do this. If you want to optimize page six of your category pages, go for it. My feeling is just that it's not the best use of your time.  

#### [0:22:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1350) |  We've been experiencing slow average response time.

Can this demote our keyword rankings? Initially, our page was crawled and the crawl stats also dropped, but now they've more than doubled. So yeah, I think that's something where you need to take a look at what exactly-- where you're seeing kind of this slow, average response time. Because there are two places where speed plays a role,  

#### [0:23:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1380) |  from our point of view. On the

one hand, there is the practical aspect of when we crawl our web page, we need to be able to get that content as quickly as possible so that we can crawl as much as possible from your website. That's more kind of a practical thing for Google bot. If your server is slow we tend to slow down our crawling as well. And then we can't crawl as much. So if you have a lot of content and your server is slow, then probably we won't get through to everything.  

#### [0:23:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1410) |  The other aspect is with regards to

what a user would see when they go to the page. And that's less kind of from the practical point of view of individual files, but more from the point of view of, when you load this page in a browser, how long does it take to actually display the full content there? So those are kind of the two aspects there. With regards to the crawl speed, the time that it takes for us to kind of fetch individual pages,  

#### [0:24:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1440) |  that's something that's, like I said, purely

from a crawl point of view, a factor. If we can't crawl as much, we can't show it in search. When it comes to the speed that a user would see in a browser, kind of the rendering speed, that's something that can play a role in ranking, because when we send people off to a website that's really slow, we've done them a little bit of a disservice by sending them somewhere where their time is being wasted.  

#### [0:24:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1470) |  So those are kind of the two

aspects there. The crawl stats in Search Console show the Google Bot side. And the speed report in Search Console, the page speed insights, lighthouse tests, they tend to show the more user-facing side, which is also reflected in the ranking. Let's see. Does Google Bot desktop and smartphone always have the text Google in it? Because many times, our Search Console crawl stats  

#### [0:25:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1500) |  don't match up with our Kibana server

log trend. And in our server logs, when I fill them for Google, it shows media partner Google, primarily, and Google Web Light, and some other user agents that look like Google Bot that don't contain the Google in them. They contain things like Chrome with a server number or something like that. So we use the Google Bot infrastructure for a number of different crawlers.  

#### [0:25:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1530) |  And that's what you're seeing in the

crawl stats report. The primary other kind of non search crawlers that you would see there are the Google Bots. What is it? The media-- you mentioned it in the question-- partner bot, which is, I believe, from AdSense. Then there's also the AdWords landing page check, I believe.  

#### [0:26:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1560) |  And then there is the Merchant Center,

kind of the product search, shopping, landing page tester, or whatever that's called. I don't know what exactly those user agents are called. We have them documented in the Help Center. So you can double-check there. So it's not only Google Bot for search, but kind of also these other requests, as well. There is also, I believe, the rendering side of things  

#### [0:26:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1590) |  might also show up there. So when

we render a page for search to determine if it has any JavaScript that we need to execute, then we also need to fetch all of those individual pages there, which would also go through the crawl stats report. One of the tricky things with the crawl stats report and the speed there in general is that, obviously, when we start crawling a lot more from your website, your web server might be a little bit slower. So we tend to balance that automatically in that,  

#### [0:27:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1620) |  if we recognize a server is getting

slower, we'll try to reduce crawling a little bit. And the other thing that plays into speed and the size, as well, is that if, for example, your website has a lot of really large content. Like perhaps it has a ton of PDFs, which are really large, and we go off and try to crawl those, then it might look like the average page is much slower, or we're crawling a lot of data from your server,  

#### [0:27:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1650) |  but actually it's just, well, you have

a lot of really large files. So we ended up crawling those. My blog is just three months old. My posts would keep appearing in the first page and then they suddenly disappear after one or two days. Why does that happen? I think that's always a little bit tricky in the beginning when we first see websites when they're fairly new, because we don't have a lot of signals for those websites.  

#### [0:28:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1680) |  And we aren't really sure how we

should show the content from those websites, kind of where those websites are relevant. We don't have a lot of information from the rest of the web that tells us, this is actually a really good website and we should show it in search. And with that, our algorithms have to make some assumptions in the beginning. And they have to kind of look at your pages and say, well, I don't really know a lot about this website, but this looks OK. Maybe I'll try to show it like this in the search results.  

#### [0:28:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1710) |  And over time, that will settle down

where we have a lot more signals from the rest of the web, and we can kind of be a little bit more deliberate in how we show things in search. So what you're seeing there is fairly common, especially for new websites, that maybe they're shown very visibly in the beginning, and then it drops off to kind of a stable state. Sometimes it's also the case that the competition is really strong and we tend not to show kind of these new pages  

![](https://i.ytimg.com/vi/wX9MKAgOG1E/maxres2.jpg)



#### [0:29:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1740) |  as visibly as we otherwise might. And

over time we'll see actually this is a really great site. We should show it even in the competitive search results a little bit more visibly. So my kind of gut feeling would be to say the first six to nine months of a website, things are going to be fluctuating. And it's up to you to make sure that the picture that you're providing is really a good picture, where it's clear to us that actually this is a really strong website,  

#### [0:29:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1770) |  and we should be showing it very

visibly in search. So improving the quality of your website. Also perhaps finding ways to promote your website independently of search so that users can still find your website. And if they really like it, they can link to it, and they can recommend it to your friends. You're kind of building up that traffic from both sides, not just throwing a website out there and hoping that Google Bot does some magic and sends you a lot of traffic. My website ranks for SEO company and location keyword phrase  

#### [0:30:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1800) |  from the last two years at the

first position at that location, and then suddenly, another website ranks for the same key phrase. And that website was not even a competitor. We analyzed the important points of that website. We concluded that the domain name is an exact key phrase and backlinks are almost the same. We also have 70% content marketing, the same as we have.  

#### [0:30:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1830) |  So my question is, how can we

beat out that website, which is ranking now on the first position for a couple of days? We've been in this industry for the last 10 years. I don't know. It's really hard to say. It might be that they're doing some things really well. It might be that you're kind of not doing things as well. It's really hard to say. But in general, some websites go up and some websites go down.  

#### [0:31:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1860) |  So maybe this website was in one

of the previous Hangouts and learned a magic trick that helped them to make it so that Google Bot can find their website better. So there is no simple solution to say, I will always be ranking first. And especially when you're saying you've been in the industry for the last 10 years, kind of that historical aspect of, our website was always ranking number one, that's something that I would just assume is not going to be the case.  

#### [0:31:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1890) |  Things will always change. So you need

to always stay on top of things. And if-- kind of the way that the web evolves in your niche is a way that requires some changes on your website, then that's something you should think about doing. And always keep testing, making sure that you're doing things that work well for users. With regards to kind of why a competitor might be able to do  

#### [0:32:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1920) |  that, that's always tricky. I would not

assume that this is only because the domain name has some keywords in it. We do use keywords and domain names in URLs. But it's a really tiny factor, and it's really something that mostly makes sense when we have very little other information about a website. So that's something where I wouldn't assume it's impossible for you to kind of beat them  

#### [0:32:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1950) |  again and get that number one ranking

again. But it does require work. And it's not something where you can find that magical meta tag that you put on your pages and suddenly everything gets better. So if you're an SEO company trying to rank for these SEO company phrases, then you probably know all of the things that you could be focusing on, that you could be working on. And if you're unsure of what you could  

#### [0:33:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=1980) |  be doing with your website, or with

this specific situation, I would definitely recommend dropping by the Webmaster Help Forum and kind of laying open what the problem is that you're looking at, the URLs that you're looking at, and asking for honest feedback from people. And if you're an SEO company, there will be some feedback in the forums saying like, oh, you're an SEO company. You should know what you should be doing. But it's still important to get a lot of that feedback  

#### [0:33:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2010) |  and to get a diverse set of

opinions about this so that you can kind of filter out which of these aspects are things that you think really matter, or that you think you'd really like to try out. Because some things you might say, I don't want to do that, or that's not something I'm good at. Or I don't want to hire someone to do that at the moment. That's totally up to you. But having that set of ideas is an important first step.  

#### [0:34:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2040) |  Are there any pros or cons for

having multiple site map files without completely filling 50,000 URLs? We're planning to implement last mod tags. But currently, we have around 200 files in our site map index, and some site maps contain fewer than 100 URLs. And none contain more than 20,000. Would Google be able to see last modified URLs in all site map files, because I'm seeing Google doesn't read all site map files every day.  

#### [0:34:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2070) |  So yes, we don't crawl all site

map files every day. That's correct. But you can have multiple site map files. That's perfectly fine. What we recommend doing when you update a site map file is to ping that site map file to us. So there is a mechanism I believe that's documented in the Help Center where you send essentially Google Bot a request saying, hey, I updated my site map file. Take a look.  

#### [0:35:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2100) |  And then we'll go off and double

check that site map file. So that's kind of what I would recommend doing there. Having fewer site map files makes this a little bit easier. So if you can fold things together, that's also fine. If you can fold things together in a way that you have the fresher content in a single site map file, and kind of the older content that hasn't changed for a while in other site map files, that also makes it a little bit easier because we can focus on that site map file for the newer URLs, for example.  

#### [0:35:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2130) |  But those are small optimizations, I would

say, compared to just generally using the last modification date. Because the last modification date does help us quite a bit. Can enhancement-- GASTON RIERA: [INAUDIBLE] warnings. JOHN MUELLER: Hi, yeah. GASTON RIERA: Can I ask a question? JOHN MUELLER: Sure. GASTON RIERA: Hey, Gaston here. I'm working on a site that has been impacted by a traffic  

#### [0:36:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2160) |  drop, organic traffic drop. We analyzed the

whole website. We've analyzed the website from the last four months. We saw there is no technical issue. There is no content issue. The site loads pretty fast. I think it's a fairly good site. Everything checks out, but we are thinking about this theory about bad neighborhood in the IP  

#### [0:36:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2190) |  address. So is there a chance that

this site being hosted on Cloudflare and sharing the IP with too many really, really sketchy websites, is that a reason for being penalized, quote, unquote. JOHN MUELLER: No, usually not. So it's really common that websites share the same IP address. GASTON RIERA: The thing is-- I'm sorry, John.  

#### [0:37:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2220) |  The thing is that usually, like you

just said, we are trying to think outside the box, because we are even asking other SEO colleagues to say, hey, what's going on here? We can't figure this out. There is no actual coincidence with Google updates. We [INAUDIBLE] updates. It's like we are tearing our hairs out. JOHN MUELLER: Yeah, so the usually is something where--  

#### [0:37:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2250) |  there are extreme cases where when the

web spam team takes a look at a website and they see that, oh, on this server there are 9,000 other sites and they're all spammy, then that's something where the web spam team might say, OK, all of these websites are spammy. And if there are one or two reasonable websites in there, that's something where that could happen that they are affected by this. With regards to everything else, like normal hosting,  

#### [0:38:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2280) |  if you're hosted with a shared hosting

provider, if you're using a CDN, there is such a broad mix of websites there that, even when someone manually looks at it and sees, oh, there are 1,000 spammy websites here, it's very obvious that there are also like 2,000 good websites there. So just because there is a significant set of sites that are spammy on the same IP address wouldn't result in the whole IP address being demoted. So it's really only kind of a really extreme case where  

#### [0:38:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2310) |  maybe a spammer is running a server,

and they have tens of thousands of websites on that server, and they're also renting it out to other people to make it look a little bit more legitimate. That's the case where maybe it could happen that we kind of miss something. GASTON RIERA: If that was the case, should we have a notification in Google Search Console? JOHN MUELLER: Usually, yeah.  

#### [0:39:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2340) |  GASTON RIERA: Usually. OK. JOHN MUELLER: Yeah,

yeah. That's something where we would do that as a manual action. And with the manual action, that would result in a notification in Search Console. And then you can go through the reconsideration and say, hey, my website is not pure spam. It's like, what are you thinking? Look at my website. It's normal. But yeah. GASTON RIERA: Awesome, thank you. I have another two questions, but I want to open if anyone wants to ask a question.  

#### [0:39:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2370) |  JOHN MUELLER: OK, let me run through

the questions that were submitted. Oh, there's still a whole bunch. But yeah. Let's see. How much time do we have. We have 15 minutes. Well, we can shift to open questions and come back to the submitted ones if we run out. Go for it. GASTON RIERA: Awesome, anyone wants to ask? BRANDON D: Yeah, hey, John. I have a question. JOHN MUELLER: Sure. BRANDON D: So I work on a pretty massive user-generated content  

#### [0:40:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2400) |  site, and remote mostly focused in the

US, but we've seen great growth in our international usage and international content. This includes content that comes from other regions that may be in English, but also is in their native language. And we've noted that, although this content has been driving a significant amount of traffic, it  

#### [0:40:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2430) |  doesn't seem to perform as well from

the standard engagement metrics, bounce rates, time on site, conversion rates. It doesn't seem like this content is resonating with users as well as the US content. And we're fearful that the international content that we're getting may be dragging down our ability to perform when it comes to the US content. So we're wondering if it would be a valid approach  

#### [0:41:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2460) |  to split the site into separate domain,

or perhaps a subdomain so that way we could do a better job of protecting our domain level kind of quality. JOHN MUELLER: I mean, you can definitely split things up into separate domains or subdomains. I think overall, assuming this international content is  

#### [0:41:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2490) |  of reasonable quality, probably you would be

better off just keeping everything together. If you're really not sure of the quality of the international content, then that's something I would try to figure out first, rather than just looking at the pure metrics of things like bounce rate and interactions on the website, time on site type of things. Maybe find some local speakers to double-check  

#### [0:42:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2520) |  the quality of that content, especially if

it's user-generated content, just to make sure that it matches what you would like to see on your website, what you'd like to kind of have published under your website's name. But in general, while you can split things off and make separate sites, I think that makes it overall almost a little bit harder for the whole website, because suddenly you have two websites that you have to work on and promote, and make sure that they're performing  

#### [0:42:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2550) |  well in search, rather than one really

big website that has kind of this mix of different kinds of content. BRANDON D: OK, and do you think that the content in the native languages could potentially be-- let's say it's all within the same topic. Do you think the aspect of language difference could be diluting the relevance for the US or English audience?  

#### [0:43:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2580) |  JOHN MUELLER: I don't think so. I

mean, what I would double-check is to make sure that this content ranks in those languages and not in English. But in general, that should be fine. BRANDON D: OK, thanks. I appreciate it. JOHN MUELLER: Sure. GASTON RIERA: So I [INAUDIBLE]. JOHN MUELLER: Sure.  

#### [0:43:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2610) |  GASTON RIERA: Continuing with this idea of

bad neighborhood, we had a theory about bad neighborhood traffic. So imagine you have a news website, but you are getting a lot of traffic from a video that runs for a porn search. So it's completely unrelated and completely different topic. So is there any chance that that traffic coming from a completely unrelated search term  

![](https://i.ytimg.com/vi/wX9MKAgOG1E/maxres3.jpg)



#### [0:44:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2640) |  is hurting the main site? Or the

other way around. If we knowing it's that page and remove that traffic, could it benefit us? Does it make sense, the question? JOHN MUELLER: Yeah. In general, that wouldn't change anything. The tricky aspect there is kind of the adult versus non-adult angle. Like when you say it's ranking for porn terms,  

#### [0:44:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2670) |  that's something where I would just make

sure that overall your website is kind of in this clear situation where it's not an adult website. If you have some pieces of content that are more adult-oriented, that's fine. But make sure that you stay clearly on the side where-- I don't know-- like the primary part of your website is clearly not adult, because what could happen otherwise  

#### [0:45:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2700) |  is that the safe search algorithms say,

oh, we think maybe this website is an adult website. And we will use safe search to filter the whole website. So with a normal news website, that's something you generally want to avoid. But if these are individual pages, that's usually fine. It's something where the biggest effect that I've seen is that it makes it really hard to look at the metrics overall. So we have this, for example, with our help forums, every now  

#### [0:45:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2730) |  and then I take a look at

our analytics for our help forums. And the overall traffic is going up and going down. And like we haven't changed anything with the forums. They're not hosted in a great way from an SEO point of view, but like we don't change a lot of things. But when you look into the details, it's clear that sometimes we're just ranking for irrelevant things. And we get a lot of traffic for those irrelevant things.  

#### [0:46:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2760) |  So for example, with the forums, sometimes

people will use XXX as a placeholder for their domain name. And suddenly, the help forum ranks for XXX, which it's like, obviously, they're looking for something else. They're not looking for the Webmaster Help Forum. I mean, I am guessing. But if you look at the overall traffic, it's like, oh, so many people are searching for our forums and coming to the forums. And when that goes away, it's like, oh, our website disappeared from search.  

#### [0:46:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2790) |  And it's not that the primary content

disappeared. It's just this big piece of irrelevant traffic disappeared. GASTON RIERA: Mhm. So is it OK to think that, ooh, there is no benefit or penalty from having that traffic? JOHN MUELLER: I'd say there is no advantage or disadvantage. It depends on what you want to achieve. Some people I've seen take this kind of situation  

#### [0:47:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2820) |  and say, OK, I'm getting a lot

of traffic for something irrelevant. Maybe I can put an affiliate link on there. And then it's like, I get this irrelevant traffic. At least they click on my affiliate link, and that's fine. Other people say I don't want this extra traffic, because it ruins my analytics. And they no index the page. So that's totally up to you. GASTON RIERA: Awesome. I have another question, but I want to let anyone else beforehand ask some question.  

#### [0:47:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2850) |  ESBEN RASMUSSEN: I have a quick question

regarding robots.txt. So isn't it correct that when a robots throws a server error 500, that that will prevent crawling of the website? JOHN MUELLER: Yes, at least temporarily. So if we see it for a longer period of time, we'll assume it's a 404 kind of thing.  

#### [0:48:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2880) |  And then we'll think, OK, we can

crawl. ESBEN RASMUSSEN: And could you be sort of a bit more specific? What's a longer period of time? How much time are we talking about usually? JOHN MUELLER: Usually-- so I don't know what is written in the spec, but I believe it's a couple of days, something around that order. ESBEN RASMUSSEN: OK. JOHN MUELLER: But that should be in the robots.txt documentation that we have on the developer site.  

#### [0:48:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2910) |  I think that was specified with kind

of the change to making it more of a standard. I think they specified that. ESBEN RASMUSSEN: OK, great, thanks. JOHN MUELLER: Sure. GASTON RIERA: So me again. JOHN MUELLER: No problem. GASTON RIERA: I'm consulting on a really, really big website that has over 10 million index pages.  

#### [0:49:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2940) |  So we found out that nearly half

of those pages are irrelevant, low-quality, or near duplicate content. This is a really big website, again. It has a massive amount of traffic. It's been around in the industry around 10 years. So we are thinking about, and progressively no indexing and getting rid of those shady places, let's say. Imagine a case when we no index 20% of the no index pages,  

#### [0:49:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=2970) |  considering that those pages don't bring any

regular traffic, and we don't lose any money, don't lose any significant traffic. Do you think we-- let me rephrase that. How much time would you think we should wait and see, expect it to change, if we should see any change? JOHN MUELLER: So I think that's generally a good idea, especially with a really large website where you can't double  

#### [0:50:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3000) |  check the quality and improve the quality

of all of those pages. I think that's usually a good idea. There are two aspects that kind of play into that. On the one hand, with regards to crawling, we'll be able to focus a lot more on the actual indexable pages, which means we can crawl them a lot faster, a lot more frequently. We can get the new content a lot faster. That's always a positive thing, especially with large, unchanging websites. And the other aspect is with the overall quality of the website.  

#### [0:50:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3030) |  And that's something that probably takes a

little bit longer. So my guess is if these are really low quality pages, then they're already not being crawled that frequently. So probably I'm just like handwaving a number, maybe three to six, nine months until you see changes in the crawling. And my guess is a little bit later still  

#### [0:51:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3060) |  you would see changes with regards to

the quality of the website. So it's really more of a long term thing, especially with a really large website, because it takes a while for us to even notice that there is a significant change. GASTON RIERA: Yeah, we thought the same. But it's always safe to ask. JOHN MUELLER: Yeah, I mean, the crawling aspect is something you can monitor fairly easily with the log files, I mean, depending on how easily the log files are. GASTON RIERA: [INAUDIBLE].  

#### [0:51:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3090) |  JOHN MUELLER: Yeah. So yeah, that's something

where we occasionally see that at SEO conferences. People will do a case study. And they will show. It's like, oh, I worked for this really large website, and I removed 30% of the pages. And these are the results that I saw. It's never been the case that I've seen that if you remove a lot of cruft pages that you have any negative effect. I think there's always going to be a positive effect, especially for websites where things continue to change.  

#### [0:52:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3120) |  So if like this content is out

there, and it never changes anymore, then crawling doesn't matter. But if you're continuously putting new pages up, and you clean out the old things, the unused things, then it makes it a lot easier. GASTON RIERA: Understood. Understood. Thank you so much, John. JOHN MUELLER: Sure. BRANDON D: Hey, John. I have one quick follow up to my previous question.  

#### [0:52:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3150) |  Let's say cost is of no concern

to us. We have the infrastructure and the resources to build out these country specific domains. Given that cost is not a factor, would we expect any maybe inherent benefit to having such kind of localized websites for international content that is both maybe in English, but is somewhat nuanced to that geography,  

#### [0:53:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3180) |  but also is in that geography's native

language? JOHN MUELLER: I don't know. It's hard to say. My general feeling is, unless you're providing something specific for users in those locations-- so not just in that language, but really for users in that location-- then probably, you'd be better off keeping it all on one domain.  

#### [0:53:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3210) |  By doing things with country code top-level

domains, if you're really providing content which is specific to their location-- I don't know. For example, maybe you have a chain of pizzerias, and people are searching for pizzerias. And they can recognize, this is from my country, so it's more relevant to me, then that's something where it would make sense. On the other hand, if-- I don't know-- your website has pizza recipes on it,  

#### [0:54:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3240) |  then the location of the website is

kind of irrelevant. Then the geotargeting aspect there wouldn't really be that critical. And I think with a lot of user-generated content, it's not the case that it's geographically relevant. It's more that it's in their language, and they can read it, which makes it helpful. But they're already going to be searching in that language. So those pages are already going to be relevant. It's not that we have to kind of add  

#### [0:54:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3270) |  the extra factor of it's nearby to

them. BRANDON D: I see. So in the case of, let's say, education, if something was related to courses that they may be taking, would then-- although, maybe it might be tied to a specific university, but there still isn't quite the need to have any physical association with that school. It's just, oh, this is content from that school  

#### [0:55:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3300) |  or from this course, do you then

think that that's enough nuance in the area of geography where it would sway a little bit more towards the pizza restaurants example, as opposed to the pizza recipe one? JOHN MUELLER: Yeah, I think that could kind of make sense, where if it's really tied to a physical location, or even if it's not tied to a location, if it's tied maybe to a certificate that's  

#### [0:55:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3330) |  relevant in that country, that's something where

kind of the geotargeting angle would make sense. BRANDON D: OK, all right. Thank you. JOHN MUELLER: Sure. GASTON RIERA: John, a completely unrelated question. Is there any chance, or do you foresee in the future Webmaster Hangouts in Spanish? JOHN MUELLER: Webmaster Hangouts in Spanish? GASTON RIERA: Yeah. JOHN MUELLER: I don't know. I don't know. I don't think we have a lot of people here  

#### [0:56:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3360) |  that speak Spanish that would be able

to do these. I don't know. GASTON RIERA: Oh, you're muted. Someone muted you. JOHN MUELLER: Whoops, OK. Maybe you can twist Gary's arm. He speaks a little bit of Spanish. He also does kind of more off-the-record Hangouts at the moment. So you can join in on one of those. I think he mentions those on Twitter every now and then.  

#### [0:56:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3390) |  GASTON RIERA: Awesome, thank you. JOHN MUELLER:

Sure. TOM: I had a question. JOHN MUELLER: Sure. TOM: We've been kind of debating internally a particular use of a XML site map, where we have a lot of content, and we have a particular set of content that we think our best content, and then  

#### [0:57:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3420) |  our kind of medium content, and our

lower content, whatever. And there's been an internal debate about whether it would actually provide a benefit to that best content if we were only submitting that best content in the site map, instead of submitting everything. Is there any validity to that? JOHN MUELLER: I don't think that would change anything. So the site map file is really only used for the crawling side, which is more the technical aspect  

#### [0:57:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3450) |  of getting content into Google. It's not

a quality factor. So it's not like an internal link where it would say, oh, you want to promote this content. It's really just purely, from a technical point of view, like this page changed. It's like, take a look at it. It's not a sign that you're saying, well, this is a good page. You should show it more visibly in search. TOM: Sounds great. Thank you. JOHN MUELLER: Sure.  

#### [0:58:00](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3480) |  All right, maybe we can take a

break here to pause the recording. I can stay on a little bit longer if any of you want to hang around. But it's always good to close out the recording at some point. Thank you all for joining in. Thanks for all of the many questions that came in. And I wish you all a great weekend. And stay safe, wash your hands, do the usual things. Take part in these virtual Hangouts  

#### [0:58:30](https://www.youtube.com/watch?v=wX9MKAgOG1E&t=3510) |  rather than joining a big group of

people. It's always a good idea. All right, see you all next time. Bye. KARL JENKINS: Thanks, John.  